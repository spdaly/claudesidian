# How Chain-of-Thought Reasoning Helps Neural Networks Compute | Quanta Magazine

![rw-book-cover](https://www.quantamagazine.org/favicon.png)
<br>
>[!note]- Readwise Information
>Title:: How Chain-of-Thought Reasoning Helps Neural Networks Compute | Quanta Magazine
>Author:: [[Ben Brubaker]]
>Type:: #Readwise/category/articles
>Published-Date:: [[2024-03-21]]
>Last-Highlighted-Date:: [[2024-03-28]]
>Readwise-Link:: https://readwise.io/bookreview/38922748
>Readwise-Source:: #Readwise/source/reader
>Source URL:: https://www.quantamagazine.org/how-chain-of-thought-reasoning-helps-neural-networks-compute-20240321/
--- 

## Linked Notes
```dataview
LIST
FROM [[How Chain-of-Thought Reasoning Helps Neural Networks Compute | Quanta Magazine by Ben Brubaker Highlights]]
```

---

## Highlights
- ‚ÄúHow we humans solve these problems is not ‚Äòstare at it and then write down the answer,‚Äô‚Äù said [Eran Malach](https://www.eranmalach.com/), a machine learning researcher at Harvard University. ‚ÄúWe actually walk through the steps.‚Äù [View Highlight](https://readwise.io/open/698957795) ^rw698957795
- Now, several teams have explored the power of chain-of-thought reasoning by using techniques from an arcane branch of theoretical computer science called computational complexity theory. [View Highlight](https://readwise.io/open/698958058) ^rw698958058
- Their technique, called chain-of-thought prompting, soon became widespread, even as researchers struggled to understand what makes it work. [View Highlight](https://readwise.io/open/696400132) ^rw696400132
- Rather than consider what happens during training, some researchers study the intrinsic capabilities of transformers by imagining that it‚Äôs possible to adjust their parameters to any arbitrary values. This amounts to treating a transformer as a special type of programmable computer. [View Highlight](https://readwise.io/open/698957816) ^rw698957816 
- See also: [[üëª ai highlighted]] 
- One such effort began in 2021, when [William Merrill](https://lambdaviking.com/), now a graduate student at New York University, was leaving a two-year fellowship at the Allen Institute for Artificial Intelligence in Seattle. While there, he‚Äôd analyzed other kinds of neural networks using techniques that seemed like a poor fit for transformers‚Äô parallel architecture. Shortly before leaving, he struck up a conversation with the Allen Institute for AI researcher [Ashish Sabharwal](https://allenai.org/team/ashishs), who‚Äôd studied complexity theory before moving into AI research. They began to suspect that complexity theory might help them understand the limits of transformers. [View Highlight](https://readwise.io/open/698957815) ^rw698957815 
- See also: [[üëª ai highlighted]] 
- Examples like this suggest that transformers wouldn‚Äôt gain much from using just a few intermediate steps. Indeed, Merrill and Sabharwal proved that chain of thought only really begins to help when the number of intermediate steps grows in proportion to the size of the input, and many problems require the number of intermediate steps to grow much larger still. [View Highlight](https://readwise.io/open/698957819) ^rw698957819 
- See also: [[üëª ai highlighted]] 
- Still, researchers caution that this sort of theoretical analysis can only reveal so much about real language models. Positive results ‚Äî proofs that transformers can in principle solve certain problems ‚Äî don‚Äôt imply that a language model will actually learn those solutions during training. [View Highlight](https://readwise.io/open/698957818) ^rw698957818 
- See also: [[üëª ai highlighted]] 
