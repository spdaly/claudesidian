# The Next Leap for AI: Why Agents Need to Learn to Believe

![rw-book-cover](https://www.oreilly.com/radar/wp-content/uploads/sites/3/2025/07/Firefly_A-robot-leaping-726843.jpg)
<br>
>[!note]- Readwise Information
>Title:: The Next Leap for AI: Why Agents Need to Learn to Believe
>Author:: [[Rajeshwari Ganesan]]
>Type:: #Readwise/category/articles
>Published-Date:: [[2025-07-17]]
>Last-Highlighted-Date:: [[2025-07-20]]
>Readwise-Link:: https://readwise.io/bookreview/53500214
>Readwise-Source:: #Readwise/source/reader
>Source URL:: https://www.oreilly.com/radar/the-next-leap-for-ai-why-agents-need-to-learn-to-believe/
--- 

## Linked Notes
```dataview
LIST
FROM [[The Next Leap for AI: Why Agents Need to Learn to Believe by Rajeshwari Ganesan Highlights]]
```

---

## Highlights
- The answer may lie in an approach that’s been quietly developing in AI research circles: the **Belief-Desire-Intention (BDI) framework**. Rooted in the philosophy of practical reasoning, BDI systems operate on three interconnected levels. Rather than hardcoding every possible scenario, this framework gives agents the cognitive architecture to reason about what they know, what they want, and what they’re committed to doing—much like humans do with the ability to handle sequences of belief changes over time, including possible consequential changes to the intention thereafter in light of new information. [View Highlight](https://readwise.io/open/916668864) ^rw916668864
- **Beliefs** represent what the agent understands about the world, including itself and others—information that may be incomplete or even incorrect but gets updated as new data arrives. **Desires** capture the agent’s motivational state, its objectives and goals, though not all can be pursued simultaneously. **Intentions** are where the rubber meets the road: the specific plans or strategies the agent commits to executing, representing the subset of desires it actively pursues. [View Highlight](https://readwise.io/open/916668860) ^rw916668860
- The real challenge lies in building and maintaining accurate beliefs. Much of what matters in human contexts—priorities, constraints, and intentions—is rarely stated outright or captured in enterprise data. Instead, these are embedded in patterns of behavior that evolve across time and situations. This is where observational learning becomes crucial. Rather than relying solely on explicit instructions or enterprise data sources, agentic AI must learn to infer priorities and constraints by watching and interpreting behavioral patterns in its environment. [View Highlight](https://readwise.io/open/916668889) ^rw916668889
