# Mixture of Experts Explained

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/media/uploaded_book_covers/profile_174804/thumbnail.png)
<br>
>[!note]- Readwise Information
>Title:: Mixture of Experts Explained
>Author:: [[huggingface.co]]
>Type:: #Readwise/category/articles
>Published-Date:: [[2023-12-11]]
>Last-Highlighted-Date:: [[2024-03-28]]
>Readwise-Link:: https://readwise.io/bookreview/39093945
>Readwise-Source:: #Readwise/source/reader
>Source URL:: https://huggingface.co/blog/moe
--- 

## Linked Notes
```dataview
LIST
FROM [[Mixture of Experts Explained by huggingface.co Highlights]]
```

---

## Highlights
- Mixture of Experts enable models to be pretrained with far less compute, which means you can dramatically scale up the model or dataset size with the same compute budget as a dense model. In particular, a MoE model should achieve the same quality as its dense counterpart much faster during pretraining. [View Highlight](https://readwise.io/open/698957607) ^rw698957607
