# A Jargon-Free Explanation of How AI Large Language Models Work

![rw-book-cover](https://cdn.arstechnica.net/wp-content/uploads/2023/07/LLM-cat-vector-space-1-760x380.jpg)
<br>
>[!note]- Readwise Information
>Title:: A Jargon-Free Explanation of How AI Large Language Models Work
>Author:: [[Timothy B. Lee and Sean Trott]]
>Type:: #Readwise/category/articles
>Published-Date:: [[2023-07-31]]
>Last-Highlighted-Date:: [[2023-08-04]]
>Readwise-Link:: https://readwise.io/bookreview/30799253
>Readwise-Source:: #Readwise/source/reader
>Source URL:: https://arstechnica.com/science/2023/07/a-jargon-free-explanation-of-how-ai-large-language-models-work/
--- 

## Linked Notes
```dataview
LIST
FROM [[A Jargon-Free Explanation of How AI Large Language Models Work by Timothy B. Lee and Sean Trott Highlights]]
```

---

## Highlights
- If you know anything about this subject, you’ve probably heard that LLMs are trained to “predict the next word” and that they require huge amounts of text to do this [View Highlight](https://readwise.io/open/574230160) ^rw574230160
- ChatGPT is built on a neural network that was trained using billions of words of ordinary language [View Highlight](https://readwise.io/open/574230178) ^rw574230178
- As a result, no one on Earth fully understands the inner workings of LLMs [View Highlight](https://readwise.io/open/574230737) ^rw574230737
