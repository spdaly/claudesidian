# Binary classification - Training

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/media/uploaded_book_covers/profile_174804/open-graph-image_Nkm15WB.png)
<br>
>[!note]- Readwise Information
>Title:: Binary classification - Training
>Author:: [[wwlpublish]]
>Type:: #Readwise/category/articles
>Published-Date:: [[]]
>Last-Highlighted-Date:: [[2024-06-05]]
>Readwise-Link:: https://readwise.io/bookreview/41219552
>Readwise-Source:: #Readwise/source/reader
>Source URL:: https://learn.microsoft.com/en-us/training/modules/fundamentals-machine-learning/5-binary-classification
--- 

## Linked Notes
```dataview
LIST
FROM [[Binary classification - Training by wwlpublish Highlights]]
```

---

## Highlights
- Classification, like regression, is a *supervised* machine learning technique; and therefore follows the same iterative process of training, validating, and evaluating models. Instead of calculating numeric values like a regression model, the algorithms used to train classification models calculate *probability* values for class assignment and the evaluation metrics used to assess model performance compare the predicted classes to the actual classes. [View Highlight](https://readwise.io/open/729650951) ^rw729650951
- Probability is measured as a value between 0.0 and 1.0, such that the *total* probability for *all* possible classes is 1.0. [View Highlight](https://readwise.io/open/729651494) ^rw729651494
- There are many algorithms that can be used for binary classification, such as *logistic regression*, which derives a *sigmoid* (S-shaped) function with values between 0.0 and 1.0 [View Highlight](https://readwise.io/open/729651662) ^rw729651662
- The arrangement of the confusion matrix is such that correct (*true*) predictions are shown in a diagonal line from top-left to bottom-right. Often, color-intensity is used to indicate the number of predictions in each cell, so a quick glance at a model that predicts well should reveal a deeply shaded diagonal trend. [View Highlight](https://readwise.io/open/729652383) ^rw729652383
- *Recall* is a metric that measures the proportion of positive cases that the model identified correctly. [View Highlight](https://readwise.io/open/729652482) ^rw729652482
- *Precision* is a similar metric to recall, but measures the proportion of predicted positive cases where the true label is actually positive. [View Highlight](https://readwise.io/open/729652492) ^rw729652492
- *F1-score* is an overall metric that combined recall and precision. [View Highlight](https://readwise.io/open/729652517) ^rw729652517
