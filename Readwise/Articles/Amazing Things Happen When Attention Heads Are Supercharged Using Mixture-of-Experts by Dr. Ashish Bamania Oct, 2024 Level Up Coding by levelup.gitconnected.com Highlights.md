# Amazing Things Happen When Attention Heads Are Supercharged Using Mixture-of-Experts | by Dr. Ashish Bamania | Oct, 2024 | Level Up Coding

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/media/uploaded_book_covers/profile_174804/1S-qY61Qk6ANJu8Js8ddQ9w.jpeg)
<br>
>[!note]- Readwise Information
>Title:: Amazing Things Happen When Attention Heads Are Supercharged Using Mixture-of-Experts | by Dr. Ashish Bamania | Oct, 2024 | Level Up Coding
>Author:: [[levelup.gitconnected.com]]
>Type:: #Readwise/category/articles
>Published-Date:: [[]]
>Last-Highlighted-Date:: [[2024-10-28]]
>Readwise-Link:: https://readwise.io/bookreview/45345643
>Readwise-Source:: #Readwise/source/reader
>Source URL:: https://levelup.gitconnected.com/amazing-things-happen-when-attention-heads-are-supercharged-using-mixture-of-experts-b55a6b9a0ac8
--- 

## Linked Notes
```dataview
LIST
FROM [[Amazing Things Happen When Attention Heads Are Supercharged Using Mixture-of-Experts | by Dr. Ashish Bamania | Oct, 2024 | Level Up Coding by levelup.gitconnected.com Highlights]]
```

---

## Highlights
- Attention is a mechanism that allows a model to focus on different parts of the input sequence when making predictions.
  The mechanism weighs the importance of each token in the sequence and captures relationships between different tokens of the sequence regardless of their distance from each other.
  This helps the model decide which tokens from the input sequence are most relevant to the token being processed. [View Highlight](https://readwise.io/open/804354484) ^rw804354484
