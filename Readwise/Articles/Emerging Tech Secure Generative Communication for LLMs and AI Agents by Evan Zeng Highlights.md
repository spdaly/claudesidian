# Emerging Tech: Secure Generative Communication for LLMs and AI Agents

![rw-book-cover](https://emtemp.gcom.cloud/ngw/globalassets/gartner-tile.jpg)
<br>
>[!note]- Readwise Information
>Title:: Emerging Tech: Secure Generative Communication for LLMs and AI Agents
>Author:: [[Evan Zeng]]
>Type:: #Readwise/category/articles
>Published-Date:: [[2024-06-12]]
>Last-Highlighted-Date:: [[2024-06-25]]
>Readwise-Link:: https://readwise.io/bookreview/41793620
>Readwise-Source:: #Readwise/source/reader
>Source URL:: https://www.gartner.com/document/5503295?ref=hp-discovery&reqid=c2c2eeec-4475-455e-9aab-6daaa9691c8d
--- 

## Linked Notes
```dataview
LIST
FROM [[Emerging Tech: Secure Generative Communication for LLMs and AI Agents by Evan Zeng Highlights]]
```

---

## Highlights
- Gartnerâ€™s 2023 AI in the Enterprise Survey shows the most common way to fulfill generative AI (GenAI) use cases is to embed large language models (LLMs) into existing applications. This will create new security attack surfaces and lead to risks such as data loss/leakage, API attacks and compromised model safety. [View Highlight](https://readwise.io/open/738253286) ^rw738253286
- But sending enterprise data out to third parties via prompt engineering brings the risk of data leakage, especially when data can be used to train LLMs. This also breaks the traditional security rules of keeping data inside the guardrails of the enterprise. GenAI deployments in general create new security attack surfaces that should be protected by new products. [View Highlight](https://readwise.io/open/738253302) ^rw738253302
