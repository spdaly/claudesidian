# Agentic Misalignment: How LLMs Could Be Insider Threats

![rw-book-cover](https://cdn.sanity.io/images/4zrzovbb/website/e7e28ca7cbe9c943af6e6041ec0f22241024c9d9-2401x1261.png)
<br>
>[!note]- Readwise Information
>Title:: Agentic Misalignment: How LLMs Could Be Insider Threats
>Author:: [[Anthropic Research]]
>Type:: #Readwise/category/articles
>Published-Date:: [[2025-06-20]]
>Last-Highlighted-Date:: [[2025-09-03]]
>Readwise-Link:: https://readwise.io/bookreview/54734001
>Readwise-Source:: #Readwise/source/reader
>Source URL:: https://www.anthropic.com/research/agentic-misalignment
--- 

## Linked Notes
```dataview
LIST
FROM [[Agentic Misalignment: How LLMs Could Be Insider Threats by Anthropic Research Highlights]]
```

---

## Highlights
- (a) suggest caution about deploying current models in roles with minimal human oversight and access to sensitive information; (b) point to plausible future risks as models are put in more autonomous roles; and (c) underscore the importance of further research into, and testing of, the safety and alignment of agentic AI models, as well as [transparency from frontier AI developers](https://www.nytimes.com/2025/06/05/opinion/anthropic-ceo-regulate-transparency.html). We are releasing our methods [publicly](https://github.com/anthropic-experimental/agentic-misalignment) to enable further research. [View Highlight](https://readwise.io/open/933870413) ^rw933870413
- What happens when these agents face obstacles to their goals? [View Highlight](https://readwise.io/open/933870466) ^rw933870466
