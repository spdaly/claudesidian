# Inside Large Language Models: How AI Really Understands Language

![rw-book-cover](https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F58e09b8a-e64f-433a-b422-d0764cdf7b4f_1792x1024.png)
<br>
>[!note]- Readwise Information
>Title:: Inside Large Language Models: How AI Really Understands Language
>Author:: [[DiamantAI]]
>Type:: #Readwise/category/articles
>Published-Date:: [[2024-11-22]]
>Last-Highlighted-Date:: [[2024-12-19]]
>Readwise-Link:: https://readwise.io/bookreview/46927838
>Readwise-Source:: #Readwise/source/reader
>Source URL:: https://diamantai.substack.com/p/inside-large-language-models-how?r=336pe4&utm_campaign=post&utm_medium=web&triedRedirect=true
--- 

## Linked Notes
```dataview
LIST
FROM [[Inside Large Language Models: How AI Really Understands Language by DiamantAI Highlights]]
```

---

## Highlights
- LLMs learn by recognizing and generating patterns in language, similar to how our brains process information. [View Highlight](https://readwise.io/open/826425487) ^rw826425487
- your brain doesn't process each word in isolation. Instead, it considers how all the words relate to each other simultaneously. This is exactly what the transformer architecture does, and it's what makes modern LLMs so powerful. [View Highlight](https://readwise.io/open/826425628) ^rw826425628
- Transformers are a type of neural network architecture that transformed natural language processing by enabling models to consider the relationships between all words in a sequence simultaneously. [View Highlight](https://readwise.io/open/826425649) ^rw826425649
- Transformers revolutionized this approach by processing entire sequences of text at once, allowing models to better capture the relationships and context of words, just as our friends can quickly understand a story by discussing it together in real-time. [View Highlight](https://readwise.io/open/826425681) ^rw826425681
- The self-attention mechanism is like a spotlight that helps focus on the most important parts of a scene. [View Highlight](https://readwise.io/open/826425818) ^rw826425818
- Multi-head attention is like having multiple experts analyze the same text, each bringing their unique expertise to the table. [View Highlight](https://readwise.io/open/826425852) ^rw826425852
- he context window (e.g., 8K, 32K, or 100K tokens) defines how much text the model can process at once: [View Highlight](https://readwise.io/open/826426021) ^rw826426021
- Large Language Models are trained on vast amounts of text from the internet, which inevitably contains biases present in human society. These biases can become embedded in the model, affecting the way it generates responses. [View Highlight](https://readwise.io/open/826426249) ^rw826426249
