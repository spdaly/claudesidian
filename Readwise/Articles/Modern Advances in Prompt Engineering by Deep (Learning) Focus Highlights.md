# Modern Advances in Prompt Engineering

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article4.6bc1851654a0.png)
<br>
>[!note]- Readwise Information
>Title:: Modern Advances in Prompt Engineering
>Author:: [[Deep (Learning) Focus]]
>Type:: #Readwise/category/articles
>Published-Date:: [[2024-04-29]]
>Last-Highlighted-Date:: [[2024-04-29]]
>Readwise-Link:: https://readwise.io/bookreview/40065739
>Readwise-Source:: #Readwise/source/reader
>Document-Tags:: [[technology]] [[Technology]] 
--- 

## Linked Notes
```dataview
LIST
FROM [[Modern Advances in Prompt Engineering by Deep (Learning) Focus Highlights]]
```

---

## Highlights
- One major reason that LLMs are so popular is because their text-to-text interface makes them incredibly simple to use. In a prior generation, solving a task with deep learning would require that we (at a minimum) finetune a model over some data to teach the model how to solve that task. [View Highlight](https://readwise.io/open/713068620) ^rw713068620
- However, effectively prompting LLMs is both an art and a science‚Äî*significant* *performance improvements can be achieved by slightly tweaking our prompting implementation or strategy*. In this overview, we will develop a comprehensive understanding of prompt engineering, beginning with basic concepts and going all the way to cutting-edge techniques that have been proposed in recent months. [View Highlight](https://readwise.io/open/713070889) ^rw713070889 
- See also: [[üëª ai highlighted]] 
- However, effectively prompting LLMs is both an art and a science‚Äî*significant* *performance improvements can be achieved by slightly tweaking our prompting implementation or strategy*. In this overview, we will develop a comprehensive understanding of prompt engineering, beginning with basic concepts and going all the way to cutting-edge techniques that have been proposed in recent months. [View Highlight](https://readwise.io/open/713070890) ^rw713070890 
- See also: [[üëª ai highlighted]] 
- Plus, most of these models were narrow experts, meaning that they would specialize in solving a single task. Due to the emergent [in-context learning](https://substack.com/redirect/2c535165-2f63-4f88-a2af-4db92867383f?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) abilities of LLMs, however, we can solve a variety of problems via a textual prompt; see above. Previously complex problem solving processes have been [abstracted into natural language](https://substack.com/redirect/194da4ee-55b2-43e7-9075-0d6fd099742f?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI)! [View Highlight](https://readwise.io/open/713070921) ^rw713070921 
- See also: [[üëª ai highlighted]] 
- When solving a problem with an LLM, however, the results that we achieve depend heavily upon the textual prompt provided to the model. For this reason, prompt engineering‚Äî*the empirical science of testing different prompts to optimize an LLM‚Äôs performance*‚Äîhas become extremely popular and impactful, resulting in the discovery of many techniques and best practices. [View Highlight](https://readwise.io/open/713070915) ^rw713070915 
- See also: [[üëª ai highlighted]] 
- Recent research on LLMs has emphasized the creation of [long context windows](https://substack.com/redirect/b66c262b-d471-4da9-8418-842823f4214b?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI), which allow the model to process more information within each prompt (e.g., more exemplars or a larger amount of context). As we will see, however, *not all LLMs pay perfect attention to their context*! The ability of an LLM to leverage information within a long context window is typically assessed via a [needle in the haystack test](https://substack.com/redirect/12b8f51d-e631-4cb1-b171-8138235e0e24?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI), which *i)* embeds a random fact within the context, *ii)* asks the model to retrieve the fact, and *iii)* repeats this test over various context lengths and positions of the fact in the context. Such a test yields a picture like the one shown below, where we can easily spot deficiencies in the context window. [View Highlight](https://readwise.io/open/713070932) ^rw713070932 
- See also: [[üëª ai highlighted]] 
- *Avoid complexity (if possible)*: complex prompting strategies are sometimes necessary (e.g., to solve multi-step reasoning problems), but we should think twice before using such approaches. Be empirical and use the established evaluation strategy to truly determine whether the complexity is necessary. [View Highlight](https://readwise.io/open/713070881) ^rw713070881 
- See also: [[üëª ai highlighted]] 
- We will now overview relevant prompting techniques once again, providing a foundation for the more complex approaches that will be introduced later in the post. As we learn about each of these techniques, however, we should keep in mind the importance of simplicity in prompt engineering. *Just because a prompting technique is more intricate or complex does not make it better than simpler strategies*! [View Highlight](https://readwise.io/open/713070942) ^rw713070942 
- See also: [[üëª ai highlighted]] 
- **Zero-shot prompting** (shown above)‚Äî*as popularized by [GPT-2](https://substack.com/redirect/43ac0a11-e9a4-44e3-abd7-0891ae070841?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI)* [2]‚Äîis one of the most basic prompting strategies that we can employ. To solve a task via zero-shot prompting, we just *i)* describe the task in the prompt and *ii)* prompt the model to solve the problem. In the case of the problem above, the task is translating a word from English to French, and we prompt the model to make this translation via the string ‚Äúcheese =>‚Äù, which prompts the model to emit the French translation of the word cheese. Several examples of zero-shot prompts are provided below. [View Highlight](https://readwise.io/open/713070947) ^rw713070947 
- See also: [[üëª ai highlighted]] 
- **Zero-shot prompting** (shown above)‚Äî*as popularized by [GPT-2](https://substack.com/redirect/43ac0a11-e9a4-44e3-abd7-0891ae070841?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI)* [2]‚Äîis one of the most basic prompting strategies that we can employ. To solve a task via zero-shot prompting, we just *i)* describe the task in the prompt and *ii)* prompt the model to solve the problem. In the case of the problem above, the task is translating a word from English to French, and we prompt the model to make this translation via the string ‚Äúcheese =>‚Äù, which prompts the model to emit the French translation of the word cheese. Several examples of zero-shot prompts are provided below. [View Highlight](https://readwise.io/open/713070948) ^rw713070948 
- See also: [[üëª ai highlighted]] 
- **Few-shot prompting** does exactly this by inserting several examples of correct problem solutions into the prompt. This strategy was popularized by [GPT-3](https://substack.com/redirect/dc720a54-6da3-4b3a-967e-9f1a4e84fb2e?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) [3], which showed that LLMs develop impressive few-shot learning abilities at scale; see above. Intuitively, few-shot learning eliminates the ambiguity of zero-shot learning by providing several examples of the expected output. As such, the model can understand the correct behavior directly from these exemplars, rather than inferring the desired behavior from the task description; see below. [View Highlight](https://readwise.io/open/713070868) ^rw713070868 
- See also: [[üëª ai highlighted]] 
- The LLM can learn from these examples provided within the prompt, a strategy commonly referred to as *‚Äúin-context learning‚Äù*; see below. However, this style of learning is not like normal training of a neural network‚Äî*the parameters of the model are not modified at all*. Rather, we put relevant information in the prompt, and the model can use this information as context for generating a better output. [View Highlight](https://readwise.io/open/713070887) ^rw713070887 
- See also: [[üëª ai highlighted]] 
- **Instruction prompting** is a more direct method of expressing the LLM‚Äôs desired output. With few-shot learning, we explain our intent to the model via concrete exemplars of a task being solved, *but these exemplars consume a lot of tokens*! Simply explaining our intent to the model in words would be much more efficient. [View Highlight](https://readwise.io/open/713070870) ^rw713070870 
- See also: [[üëª ai highlighted]] 
- **Instruction prompting** is a more direct method of expressing the LLM‚Äôs desired output. With few-shot learning, we explain our intent to the model via concrete exemplars of a task being solved, *but these exemplars consume a lot of tokens*! Simply explaining our intent to the model in words would be much more efficient. [View Highlight](https://readwise.io/open/713070871) ^rw713070871 
- See also: [[üëª ai highlighted]] 
- As shown by InstructGPT [6], however, we can align models to be much better at following instructions via a combination of [supervised finetuning (SFT)](https://substack.com/redirect/e6aaa873-dbb5-4f0e-b76e-12d7a70e8a65?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) and [reinforcement learning from human feedback (RLHF)](https://substack.com/redirect/05d26988-02c6-453f-8e04-9ead18c31e2c?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI). We see in the figure above that this strategy can improve instruction following, as well as other key properties of the LLM (e.g., factuality and constraint following). [View Highlight](https://readwise.io/open/713070926) ^rw713070926 
- See also: [[üëª ai highlighted]] 
- Given recent advancements in LLM alignment, instruction prompting‚Äî*which can even be combined with few-shot prompting [7]*‚Äîis a highly effective approach that is commonly used in practical applications. In fact, several popular prompting strategies (e.g., [role prompting](https://substack.com/redirect/14a9c65f-6876-42e2-9cb0-c9f004c97206?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI), [specifying an audience](https://substack.com/redirect/5ad8e0dc-e1f6-44fe-bc2c-b79a10ddad0e?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI), or [tool usage](https://substack.com/redirect/88d520eb-a2cd-478f-88e3-276200e0a4f1?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) to name a few) are just more specific versions of instruction prompting! When writing instructions, we should be clear and precise to ensure the best possible results. [View Highlight](https://readwise.io/open/713070975) ^rw713070975 
- See also: [[üëª ai highlighted]] 
- **Chain of Thought (CoT) prompting [10]** elicits reasoning capabilities in LLMs by inserting a chain of thought (i.e., a series of intermediate reasoning steps) into exemplars within a model‚Äôs prompt; see above. By augmenting each exemplar with a chain of thought, the model learns (via in-context learning) to generate a similar chain of thought prior to outputting the final answer for the problem in question. Interestingly, we see in [10] that sufficiently large models (i.e., >100B parameters) benefit heavily from this approach on arithmetic, commonsense and symbolic reasoning tasks‚Äî*explicitly explaining the underlying reasoning process for solving a problem actually makes the model more effective at reasoning*. [View Highlight](https://readwise.io/open/713070901) ^rw713070901 
- See also: [[üëª ai highlighted]] 
- The implementation of CoT prompting is simple. Instead of each few-shot exemplar having only an input and output, exemplars are triplets of the form `(input, chain of thought, output)`; see above. The major downside of this approach is that we must manually (or synthetically) curate exemplars that include a full rationale for the solution to a problem, which can be expensive and/or time consuming. As such, many papers focus upon eliminating the dependence of CoT prompting upon human-written rationales! [View Highlight](https://readwise.io/open/713070920) ^rw713070920 
- See also: [[üëª ai highlighted]] [[chain of thought]] [[prompt engineering]] 
- **CoT variants.** Due to the effectiveness and popularity of CoT prompting, numerous extensions of this approach have been proposed. For example, zero-shot CoT [11] prompting eliminates few-shot exemplars and instead encourages the model to generate a problem-solving rationale by appending the words *‚ÄúLet‚Äôs think step by step.‚Äù* to the end of the prompt. [View Highlight](https://readwise.io/open/713070970) ^rw713070970 
- See also: [[üëª ai highlighted]] 
- **CoT variants.** Due to the effectiveness and popularity of CoT prompting, numerous extensions of this approach have been proposed. For example, zero-shot CoT [11] prompting eliminates few-shot exemplars and instead encourages the model to generate a problem-solving rationale by appending the words *‚ÄúLet‚Äôs think step by step.‚Äù* to the end of the prompt. [View Highlight](https://readwise.io/open/713070973) ^rw713070973 
- See also: [[üëª ai highlighted]] 
- Least-to-most prompting [13] goes beyond CoT prompting by explicitly breaking down a complex problem into multiple parts; see above. Each sub-problem is solved individually, and the solution to each sub-problem is passed as context for solving the next sub-problem. Once we have reached the final sub-problem, we can use the context of prior solutions to output a final answer to the question. [View Highlight](https://readwise.io/open/713070905) ^rw713070905 
- See also: [[üëª ai highlighted]] 
- During exploration, the LLM generates many thoughts and continually evaluates its progress toward a final solution via natural language (i.e., we just prompt the model!). By leveraging the model‚Äôs self-evaluation of its own progress towards a final solution, we can power the exploration process with widely-used search algorithms (e.g., [breadth-first search or depth-first search](https://substack.com/redirect/6edb2786-3626-447b-b1a3-6338f9264bec?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI)), allowing lookahead and backtracking to be performed within the problem-solving process. Check out [this overview](https://substack.com/redirect/d9e9245b-51fb-47d6-9c30-b0f2e15a421c?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) for a more detailed explanation of ToT prompting. [View Highlight](https://readwise.io/open/713070956) ^rw713070956 
- See also: [[üëª ai highlighted]] 
- Rather, we can re-use thoughts or even recurse through a sequence of several thoughts when deriving a solution; see above. Multiple graph-based prompting strategies have been proposed (see [here](https://substack.com/redirect/509b6daa-2ad0-4d4c-bacc-311d2680b3cf?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) for more details) [35, 36]. However, these prompting techniques‚Äî*as well as ToT prompting*‚Äîhave been criticized for their lack of practicality. Namely, solving a reasoning problem with GoT prompting could potentially require a massive number of inference steps from the LLM! [View Highlight](https://readwise.io/open/713070925) ^rw713070925 
- See also: [[üëª ai highlighted]] 
- **Retrieval Augmented Generation (RAG) [37]** (shown above), though not purely a prompting technique, is a widely used strategy that improves the quality of an LLM‚Äôs output by retrieving relevant context to include in the prompt. To retrieve useful context, we can use just use existing search techniques; e.g., pure vector search or a hybrid search engine. [View Highlight](https://readwise.io/open/713070878) ^rw713070878 
- See also: [[üëª ai highlighted]] 
- **Generated knowledge prompting [39]** is an interesting alternative to RAG that uses an LLM to generate relevant context to include in the prompt instead of retrieving this context from an external database; see above. Despite being very simple and having positive performance indications, this approach (obviously) lacks in reliability due to the tendency of LLMs to hallucinate information. [View Highlight](https://readwise.io/open/713070940) ^rw713070940 
- See also: [[üëª ai highlighted]] 
- Although we have covered a variety of prompting techniques so far, many papers have been published recently that both expand upon these methods and explore completely new styles of prompts for solving complex problems. Here, we have separated this work into several categories based upon the topic or focus: [View Highlight](https://readwise.io/open/713070934) ^rw713070934 
- See also: [[üëª ai highlighted]] 
- **Auto-CoT [15].** CoT prompting uses intermediate reasoning steps to solve complex problems, and there are two ways we can elicit these reasoning steps within an LLM‚Äôs output (depicted above): [View Highlight](https://readwise.io/open/713070914) ^rw713070914 
- See also: [[üëª ai highlighted]] 
- Although LLMs are decent zero-shot reasoners, providing concrete exemplars consistently yields better performance with CoT prompting. However, this strategy also requires human annotators‚Äî*or the prompt engineer*‚Äîto craft manual demonstrations of rationales used to answer each question. Crafting these manual demonstrations is time consuming, but it can be avoided! [View Highlight](https://readwise.io/open/713070917) ^rw713070917 
- See also: [[üëª ai highlighted]] 
- **Complexity-Based Prompting [16].** Given that CoT prompting relies upon selecting demonstrations of problem-solving rationales to include in the prompt, we might wonder: *How do we best select these demonstrations?* In [16], authors show that selecting demonstrations based on their complexity is a good heuristic. We can measure the complexity of a demonstration by simply counting the number of steps present within the chain of thought, where individual steps are separated by newline characters (`\n`). The complexity-based prompting approach proposed in [16] advocates sampling demonstrations with the highest complexity. [View Highlight](https://readwise.io/open/713070954) ^rw713070954 
- See also: [[üëª ai highlighted]] 
- Such an approach allows the LLM to iteratively refine its answer over several passes, using its prior output as context during the process. Additionally, PHP is fully compatible with CoT prompting and self-consistency‚Äî*we can combine these techniques to further improve performance*. In experiments, PHP improves the performance of GPT-3.5 in comparison to a complexity-based prompting strategy, and using PHP with GPT-4 yields state-of-the-art performance on several notable datasets (e.g., [SVAMP](https://substack.com/redirect/6010f9a0-ca7c-435b-b2a5-64e4bc69e38f?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI), GSM8K, [AQuA](https://substack.com/redirect/b659e42c-0ff8-4fc9-848d-5e32c4f7524c?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI), and [MATH](https://substack.com/redirect/ec37ae9d-4ee2-4c42-a8fc-b3d4000aafd4?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI)). [View Highlight](https://readwise.io/open/713070879) ^rw713070879 
- See also: [[üëª ai highlighted]] 
- **Decomposed Prompting (DecomP) [18]** tries to address the difficulty of solving multi-step reasoning problems with complex steps via prompting. As tasks become more complex, few-shot prompting (i.e., showing a few examples of a correct solution) will fall short. However, we can do better by decomposing complex tasks into sub-tasks that can be solved independently via prompting. In particular, authors in [18] propose a prompting framework with two components: [View Highlight](https://readwise.io/open/713070931) ^rw713070931 
- See also: [[üëª ai highlighted]] 
- Instead of solving all sub-tasks with an LLM, we can also use other symbolic systems (e.g., a task-specific model, retrieval mechanism, etc.). [View Highlight](https://readwise.io/open/713070929) ^rw713070929 
- See also: [[üëª ai highlighted]] 
- Within Decomp, sub-tasks are iteratively generated by the decomposer, solved, and returned (with relevant output) to the decomposer to generate the next sub-task. The decomposer will continue to generate sub-tasks, acting as a controller for the reasoning process, until the end-of-question `[EOQ]` marker is generated, signifying that the final answer has been produced; see below. Overall, DecomP can be thought of as a more general/flexible version of least-to-most prompting. [View Highlight](https://readwise.io/open/713070955) ^rw713070955 
- See also: [[üëª ai highlighted]] 
- **Hypotheses-to-Theories [29].** Reasoning abilities can be elicited within LLMs by prompting the model with example rationales that decompose a complex task into simple steps. However, the model may hallucinate when producing output and performance is poor on tasks that go beyond conventional or common knowledge. Put simply, *problems occur when there is a mismatch between the LLM‚Äôs knowledge base and the knowledge required to solve a task*. To solve this problem, we need a prompting approach that empowers the LLM to discover and apply necessary knowledge when solving complex reasoning problems. [View Highlight](https://readwise.io/open/713070880) ^rw713070880 
- See also: [[üëª ai highlighted]] 
- Inspired by the scientific discovery process of humans, authors in [29] propose a prompting technique, called Hypotheses-to-Theories (HtT) prompting, that follows a strategy of freely proposing (potentially incorrect) hypotheses, only keeping those that can be verified empirically, and using these verified hypothesis to solve a problem. At a high level, the goal of this strategy is to learn a rule library for the LLM that can be used for problem solving. More concretely, HtT prompting (depicted above) is comprised of two steps: [View Highlight](https://readwise.io/open/713070923) ^rw713070923 
- See also: [[üëª ai highlighted]] 
- Inspired by the scientific discovery process of humans, authors in [29] propose a prompting technique, called Hypotheses-to-Theories (HtT) prompting, that follows a strategy of freely proposing (potentially incorrect) hypotheses, only keeping those that can be verified empirically, and using these verified hypothesis to solve a problem. At a high level, the goal of this strategy is to learn a rule library for the LLM that can be used for problem solving. More concretely, HtT prompting (depicted above) is comprised of two steps: [View Highlight](https://readwise.io/open/713070924) ^rw713070924 
- See also: [[üëª ai highlighted]] 
- *Induction*: The LLM is asked to generate and verify rules over a set of training examples. Rules that appear often and frequently produce a correct answer are collected to form a rule library. [View Highlight](https://readwise.io/open/713070951) ^rw713070951 
- See also: [[üëª ai highlighted]] 
- By using a rule set during reasoning, HtT prompting reduces the probability of hallucinations. Such a finding is verified across both numerical and relational reasoning tasks, where HtT prompting is shown to provide a 11-27% absolute improvement in accuracy compared to prior prompting techniques (e.g., CoT prompting). Interestingly, the rules generated by HtT prompting are also interpretable and even transferable to different (but similar) problems. [View Highlight](https://readwise.io/open/713070913) ^rw713070913 
- See also: [[üëª ai highlighted]] 
- From here, we can simply finetune an LLM over this data. The model will learn to generate and process calls to necessary APIs directly within the textual sequence that it generates. In this case, handling API calls in an inline manner is simple because we only consider APIs with textual input and output; see below. [View Highlight](https://readwise.io/open/713070922) ^rw713070922 
- See also: [[üëª ai highlighted]] 
- The planner, which is implemented with an LLM, uses natural language to generate calls to external tools (e.g., `image_captioner` or `query_generator`). We can identify these tools via simple string matching, and the sequence of tools outputted by the planner forms a natural language program that can be executed by calling each of the corresponding task-specific modules. Examples of prompts used for the planner and a task-specific module are shown below. [View Highlight](https://readwise.io/open/713070959) ^rw713070959 
- See also: [[üëª ai highlighted]] 
- To teach the controller when to use certain tools, we include tool descriptions and usage examples within a few-shot prompt, which can easily be extended to new tools and modules. Because we leverage the planner‚Äôs in-context learning abilities to generate a solution, no training or curated rules are required to solve real-world queries. Rather, we simply provide examples of available tools to the LLM, which can then use this information to infer a sequence of tools that can be executed to yield the correct final response to a query. Going further, this tool sequence is human readable and can be easily debugged by a human user. [View Highlight](https://readwise.io/open/713070938) ^rw713070938 
- See also: [[üëª ai highlighted]] 
- In experiments, Chameleon is applied to two complex, multi-modal (i.e., meaning that both text and images are involved) reasoning tasks with GPT-4: [ScienceQA](https://substack.com/redirect/8f85d4c5-3722-4bcd-982b-602db4f046e6?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) and [TabMWP](https://substack.com/redirect/a9706c48-b1c5-48a3-9ff0-0877570f7b28?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI). Chameleon achieves new state-of-the-art performance of 86.54% on ScienceQA, outperforming CoT prompting with GPT-4 and GPT-3 by 2.55% and 11.37%, respectively. On TabMWP, we see a similar improvement with Chameleon, which achieves an accuracy of 98.78%. However, it should be noted that Chameleon‚Äôs effectiveness is powered by GPT-4‚Äôs ability to infer constraints and construct rational/consistent plans for solving complex reasoning problems. [View Highlight](https://readwise.io/open/713070912) ^rw713070912 
- See also: [[üëª ai highlighted]] 
- First, authors use a self-instruct approach to generate a tool usage dataset by prompting a powerful teacher model (i.e., ChatGPT) to create examples of relevant tools being used. Within the prompts, both visual content‚Äî*captions and bounding boxes extracted from an image*‚Äîand tool descriptions are included. The teacher leverages this information to generate tool-related instructions that can be used to process multimodal information and solve problems; see above. [View Highlight](https://readwise.io/open/713070910) ^rw713070910 
- See also: [[üëª ai highlighted]] 
- Once the dataset is generated, [Low-Rank Adaptation (LoRA)](https://substack.com/redirect/a308e4d4-a478-4519-8f39-29c4f580d896?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) can be used to easily finetune an open-source LLM to solve a range of visual problems with the help of multimodal tools. In [20], this approach is shown to improve accuracy of calls that are made by the LLM to known tools (i.e., those included in the finetuning dataset), as well as improve the model‚Äôs ability to generalize to new tools in a zero-shot manner. A direct comparison of GPT4Tools to prior work on integrating LLMs with external tools is provided within the table above. [View Highlight](https://readwise.io/open/713070935) ^rw713070935 
- See also: [[üëª ai highlighted]] 
- To solve this issue, authors in [30] construct a dataset with examples of using over 1,600 different model APIs using self-instruct [21]. Within each example, both the prompt and relevant documentation are used as context to generate an output. In other words, this is a retrieval-aware finetuning process (similar to [RAFT](https://substack.com/redirect/edc941ae-6d8d-40e5-9d2f-1d0b3222e8d3?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI)); see above. [View Highlight](https://readwise.io/open/713070958) ^rw713070958 
- See also: [[üëª ai highlighted]] 
- **HuggingGPT [31]** is pretty similar to Gorilla in that it explores the integration of LLMs with specialized deep learning models (e.g., for image recognition, video detection, text classification, and much more) via a tool usage approach. The LLM serves as the ‚Äúbrain‚Äù of a problem solving system, which plans how to solve a problem and coordinates efforts between different deep learning models that solve necessary subtasks for this problem. [View Highlight](https://readwise.io/open/713070906) ^rw713070906 
- See also: [[üëª ai highlighted]] 
- In CoT prompting, we rely upon the LLM to solve both of these steps, *but these models only excel at solving the first step*! In fact, generating an incorrect answer despite outputting a correct rationale is a common failure case for LLMs. To solve this problem, we can teach the model to output a rationale in the form of interleaved language and code (e.g., a Python program with useful comments). Then, we can generate a final answer by simply executing the provided code! [View Highlight](https://readwise.io/open/713070961) ^rw713070961 
- See also: [[üëª ai highlighted]] 
- We can execute the code from the rationale (using a sandboxed Python environment) to generate a reliable final solution‚Äî*the process of actually generating the solution is delegated to a code interpreter*. In [40], we see that an LLM that has been sufficiently trained on code (e.g., [Codex](https://substack.com/redirect/4ce337e8-8c92-4ac2-9206-05804c43e938?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI)) can be taught to solve problems in this way using a few-shot learning approach. [View Highlight](https://readwise.io/open/713070967) ^rw713070967 
- See also: [[üëª ai highlighted]] 
- Given the recent popularity of RAG and emphasis upon [long context windows](https://substack.com/redirect/0a3a145e-adfa-4407-81e6-5a1da5edf186?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) within state-of-the-art LLMs, understanding how these models process the context provided in their prompts is important. Luckily, recent research has studied the topics of context windows and in-context learning in depth, resulting in several interesting takeaways that are relevant to prompt engineering. [View Highlight](https://readwise.io/open/713070866) ^rw713070866 
- See also: [[üëª ai highlighted]] 
- **Large Language Models Can Be Easily Distracted by Irrelevant Context [22].** When prompting a language model, we usually include only relevant context and information within the prompt. However, in real-world applications, the model‚Äôs prompt usually contains contextually similar information that may or may not be relevant to the particular problem being solved. With this in mind, we might wonder: *Does adding irrelevant context to the prompt have negative side effects?* [View Highlight](https://readwise.io/open/713070960) ^rw713070960 
- See also: [[üëª ai highlighted]] 
- In [22], authors study the distractibility of modern LLMs, finding that the performance of these models can drastically deteriorate when irrelevant context is included in the prompt. To measure LLM distractibility, authors introduce a new Grade-School Math with Irrelevant Context (GSM-IC) dataset, which contains arithmetic reasoning problems with irrelevant information in the problem description; see above. Then, we can measure whether an LLM is distracted by irrelevant context by simply testing if the addition of an irrelevant sentence to the model‚Äôs prompt changes the resulting solution to a problem. [View Highlight](https://readwise.io/open/713070873) ^rw713070873 
- See also: [[üëª ai highlighted]] 
- This visualization shows us that LLMs pay the most attention to information at the beginning and end of their context. When relevant information is in the middle of the context, model performance degrades significantly‚Äî*the information is ‚Äúlost in the middle‚Äù*. In fact, GPT-3.5-Turbo performs better without any relevant context on multi-document QA tasks than when relevant documents are placed in the middle of the context. [View Highlight](https://readwise.io/open/713070882) ^rw713070882 
- See also: [[üëª ai highlighted]] 
- **Large Language Models Are Latent Variable Models [24].** Although we know that LLMs have in-context learning abilities, it is unclear how these abilities emerge from standard language model pretraining. Additionally, in-context learning is generally sensitive to the choice and format of examples used for few-shot learning. Certain demonstrations are effective examples for the model, while others are not. [View Highlight](https://readwise.io/open/713070869) ^rw713070869 
- See also: [[üëª ai highlighted]] 
- In [24], authors view LLMs in the lens of simple [topic](https://substack.com/redirect/57e692b8-4cf5-4362-92f5-6c46f17c6a03?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) / [latent variable](https://substack.com/redirect/4302f826-1934-4c8a-9a9b-d2af653f840a?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) models that relate the generation of new tokens to prior tokens observed by the language model. The details can be found in the paper, but at a high level this formulation allows us to theoretically describe the language model‚Äôs output with respect to the format and task information used within the model‚Äôs input prompt. [View Highlight](https://readwise.io/open/713070928) ^rw713070928 
- See also: [[üëª ai highlighted]] 
- From this formulation, authors develop a practical technique for selecting the best possible few-shot exemplars that uses a smaller language model to measure the [posterior probability](https://substack.com/redirect/30b097a4-d1dc-4524-87b8-9dc24bdafa4f?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI) of the model‚Äôs input‚Äî*this tells us the likelihood of different input exemplars based upon the model‚Äôs input and parameters*. We can use exemplars selected with the smaller LLM for in-context learning with a larger model (see above), which is found to yield a practical benefit. Put simply, this paper proposes an interesting (and relatively simple) theoretical view of in-context learning that can be used in practice to select better few-shot exemplars. [View Highlight](https://readwise.io/open/713070918) ^rw713070918 
- See also: [[üëª ai highlighted]] 
- We see in [25] that a more efficient decoding strategy can be devised by mimicking the thinking and writing process of humans without requiring any changes to the model, system, or hardware. In particular, humans tend to plan an outline for what they want to write, then fill in details for each element of their outline. *This is not a purely sequential process*‚Å∂! Inspired by this idea, authors in [25] propose Skeleton-of-Thought (SoT) prompting (see above), which has two steps: [View Highlight](https://readwise.io/open/713070872) ^rw713070872 
- See also: [[üëª ai highlighted]] 
- **Directional Stimulus Prompting [27].** Given the computational expense of finetuning, prompting is usually the easiest way to solve a task with an LLM. However, prompting has limitations‚Äî*guiding the LLM towards generating output with our desired content or style can be difficult*. To solve this issue, authors in [27] propose directional stimulus prompting (DSP), which introduces a ‚Äúdirectional stimulus‚Äù into the LLM‚Äôs prompt as shown in the figure above. [View Highlight](https://readwise.io/open/713070885) ^rw713070885 
- See also: [[üëª ai highlighted]] 
- To study this tradeoff in information density, authors in [28] propose chain of density (CoD) prompting, which starts by generating a summary via a vanilla prompt to GPT-4. From here, CoD prompting is used to iteratively add additional entities into the summary while keeping the length of the summary fixed, thus increasing the summary‚Äôs information density. Interestingly, we see in [28] that humans prefer summaries that are almost as dense as human-written summaries, but more dense than those generated by a vanilla prompt to GPT-4. By using CoD prompting, we can explore this tradeoff and generate higher-quality summaries. [View Highlight](https://readwise.io/open/713070874) ^rw713070874 
- See also: [[üëª ai highlighted]] 
- Within this overview, we have learned about everything from the basics of prompt engineering to cutting-edge techniques that have been proposed in the last two months! This post contains a massive amount of information, but many of the techniques we have seen are slight variations that leverage the same core prompt components: *instructions, exemplars, context, and problem-solving rationales.* Plus, we should recall the prompt engineering strategy proposed at the outset: [View Highlight](https://readwise.io/open/713070861) ^rw713070861 
- See also: [[üëª ai highlighted]] 
- Many problems can be solved via simple instruction and few-shot prompts. For complex reasoning problems, it may be necessary to use a more advanced strategy like CoT prompting with self-consistency. Additionally, we have seen a variety of prompting strategies that are useful for specific problem domains (e.g., PoT prompting for math problems or CoD prompting for summarization). Although being aware of these techniques is useful, their use cases are relatively rare, and we should only use them if we see a clear and measurable performance impact. [View Highlight](https://readwise.io/open/713070950) ^rw713070950 
- See also: [[üëª ai highlighted]] 
- [2] Radford, Alec, et al. "Language Models are Unsupervised Multitask Learners." [View Highlight](https://readwise.io/open/713070876) ^rw713070876 
- See also: [[üëª ai highlighted]] 
- [4] Work, What Makes In-Context Learning. "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?." [View Highlight](https://readwise.io/open/713070974) ^rw713070974 
- See also: [[üëª ai highlighted]] 
- [5] Zhao, Zihao, et al. "Calibrate before use: Improving few-shot performance of language models." *International conference on machine learning*. PMLR, 2021. [View Highlight](https://readwise.io/open/713070944) ^rw713070944 
- See also: [[üëª ai highlighted]] 
- [6] Ouyang, Long, et al. "Training language models to follow instructions with human feedback." *Advances in neural information processing systems* 35 (2022): 27730-27744. [View Highlight](https://readwise.io/open/713070962) ^rw713070962 
- See also: [[üëª ai highlighted]] 
- [7] Ye, Seonghyeon, et al. "Investigating the effectiveness of task-agnostic prefix prompt for instruction following." *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 38. No. 17. 2024. [View Highlight](https://readwise.io/open/713070941) ^rw713070941 
- See also: [[üëª ai highlighted]] 
- [8] Thoppilan, Romal, et al. "Lamda: Language models for dialog applications." *arXiv preprint arXiv:2201.08239* (2022). [View Highlight](https://readwise.io/open/713070963) ^rw713070963 
- See also: [[üëª ai highlighted]] 
- [9] Rae, Jack W., et al. "Scaling language models: Methods, analysis & insights from training gopher." *arXiv preprint arXiv:2112.11446* (2021). [View Highlight](https://readwise.io/open/713070964) ^rw713070964 
- See also: [[üëª ai highlighted]] 
- [10] Wei, Jason, et al. "Chain-of-thought prompting elicits reasoning in large language models." *Advances in neural information processing systems* 35 (2022): 24824-24837. [View Highlight](https://readwise.io/open/713070933) ^rw713070933 
- See also: [[üëª ai highlighted]] 
- [11] Kojima, Takeshi, et al. "Large language models are zero-shot reasoners." *arXiv preprint arXiv:2205.11916* (2022). [View Highlight](https://readwise.io/open/713070919) ^rw713070919 
- See also: [[üëª ai highlighted]] 
- [13] Zhou, Denny, et al. "Least-to-most prompting enables complex reasoning in large language models." *arXiv preprint arXiv:2205.10625* (2022). [View Highlight](https://readwise.io/open/713070884) ^rw713070884 
- See also: [[üëª ai highlighted]] 
- [14] Yao, Shunyu, et al. "Tree of thoughts: Deliberate problem solving with large language models." *arXiv preprint arXiv:2305.10601* (2023). [View Highlight](https://readwise.io/open/713070966) ^rw713070966 
- See also: [[üëª ai highlighted]] 
- [16] Fu, Yao, et al. "Complexity-based prompting for multi-step reasoning." *The Eleventh International Conference on Learning Representations*. 2022. [View Highlight](https://readwise.io/open/713070888) ^rw713070888 
- See also: [[üëª ai highlighted]] 
- [17] Zheng, Chuanyang, et al. "Progressive-hint prompting improves reasoning in large language models." *arXiv preprint arXiv:2304.09797* (2023). [View Highlight](https://readwise.io/open/713070969) ^rw713070969 
- See also: [[üëª ai highlighted]] 
- [18] Khot, Tushar, et al. "Decomposed prompting: A modular approach for solving complex tasks." *arXiv preprint arXiv:2210.02406* (2022). [View Highlight](https://readwise.io/open/713070945) ^rw713070945 
- See also: [[üëª ai highlighted]] 
- [19] Lu, Pan, et al. "Chameleon: Plug-and-play compositional reasoning with large language models." *Advances in Neural Information Processing Systems* 36 (2024). [View Highlight](https://readwise.io/open/713070875) ^rw713070875 
- See also: [[üëª ai highlighted]] 
- [20] Yang, Rui, et al. "Gpt4tools: Teaching large language model to use tools via self-instruction." *Advances in Neural Information Processing Systems* 36 (2024). [View Highlight](https://readwise.io/open/713070946) ^rw713070946 
- See also: [[üëª ai highlighted]] 
- [21] Wang, Yizhong, et al. "Self-instruct: Aligning language models with self-generated instructions." *arXiv preprint arXiv:2212.10560* (2022). [View Highlight](https://readwise.io/open/713070904) ^rw713070904 
- See also: [[üëª ai highlighted]] 
- [22] Shi, Freda, et al. "Large language models can be easily distracted by irrelevant context." *International Conference on Machine Learning*. PMLR, 2023. [View Highlight](https://readwise.io/open/713070908) ^rw713070908 
- See also: [[üëª ai highlighted]] 
- [23] Liu, Nelson F., et al. "Lost in the middle: How language models use long contexts." *Transactions of the Association for Computational Linguistics* 12 (2024): 157-173. [View Highlight](https://readwise.io/open/713070867) ^rw713070867 
- See also: [[üëª ai highlighted]] 
- [24] Wang, Xinyi, et al. "Large language models are latent variable models: Explaining and finding good demonstrations for in-context learning." *Advances in Neural Information Processing Systems* 36 (2024). [View Highlight](https://readwise.io/open/713070883) ^rw713070883 
- See also: [[üëª ai highlighted]] 
- [26] Diao, Shizhe, et al. "Active prompting with chain-of-thought for large language models." *arXiv preprint arXiv:2302.12246* (2023). [View Highlight](https://readwise.io/open/713070860) ^rw713070860 
- See also: [[üëª ai highlighted]] 
- [27] Li, Zekun, et al. "Guiding large language models via directional stimulus prompting." *Advances in Neural Information Processing Systems* 36 (2024). [View Highlight](https://readwise.io/open/713070911) ^rw713070911 
- See also: [[üëª ai highlighted]] 
- [29] Zhu, Zhaocheng, et al. "Large language models can learn rules." *arXiv preprint arXiv:2310.07064* (2023). [View Highlight](https://readwise.io/open/713070863) ^rw713070863 
- See also: [[üëª ai highlighted]] 
- [29] Zhu, Zhaocheng, et al. "Large language models can learn rules." *arXiv preprint arXiv:2310.07064* (2023). [View Highlight](https://readwise.io/open/713070864) ^rw713070864 
- See also: [[üëª ai highlighted]] 
- [32] Schick, Timo, et al. "Toolformer: Language models can teach themselves to use tools." *arXiv preprint arXiv:2302.04761* (2023). [View Highlight](https://readwise.io/open/713070968) ^rw713070968 
- See also: [[üëª ai highlighted]] 
- [34] Chen, Shouyuan, et al. "Extending context window of large language models via positional interpolation." *arXiv preprint arXiv:2306.15595* (2023). [View Highlight](https://readwise.io/open/713070877) ^rw713070877 
- See also: [[üëª ai highlighted]] 
- [36] Yao, Yao, Zuchao Li, and Hai Zhao. "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models." *arXiv preprint arXiv:2305.16582* (2023). [View Highlight](https://readwise.io/open/713070952) ^rw713070952 
- See also: [[üëª ai highlighted]] 
- [36] Yao, Yao, Zuchao Li, and Hai Zhao. "Beyond Chain-of-Thought, Effective Graph-of-Thought Reasoning in Large Language Models." *arXiv preprint arXiv:2305.16582* (2023). [View Highlight](https://readwise.io/open/713070953) ^rw713070953 
- See also: [[üëª ai highlighted]] 
- [37] Lewis, Patrick, et al. "Retrieval-augmented generation for knowledge-intensive nlp tasks." *Advances in Neural Information Processing Systems* 33 (2020): 9459-9474. [View Highlight](https://readwise.io/open/713070907) ^rw713070907 
- See also: [[üëª ai highlighted]] 
- [40] Gao, Luyu, et al. "PAL: Program-aided Language Models." *arXiv preprint arXiv:2211.10435* (2022). [View Highlight](https://readwise.io/open/713070916) ^rw713070916 
- See also: [[üëª ai highlighted]] 
- [41] Chen, Wenhu, et al. "Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks." *arXiv preprint arXiv:2211.12588* (2022). [View Highlight](https://readwise.io/open/713070957) ^rw713070957 
- See also: [[üëª ai highlighted]] 
- [43] Zhang, Zhuosheng, et al. "Multimodal chain-of-thought reasoning in language models." *arXiv preprint arXiv:2302.00923* (2023). [View Highlight](https://readwise.io/open/713070937) ^rw713070937 
- See also: [[üëª ai highlighted]] 
- 5Many researchers have argued that this majority vote strategy is insufficient for solving complex problems. This has led to a lot of research on prompt ensembles and other self-consistency variants that was in-depth [here](https://substack.com/redirect/d4ed5809-6000-427a-9268-a5b4012e838d?j=eyJ1IjoiMmc3a2xqIn0.uzdTzNmnO6ouD8B9Cxd6K3-0mXi5lgUXgnQWQGjtJkI). [View Highlight](https://readwise.io/open/713070927) ^rw713070927 
- See also: [[üëª ai highlighted]] 
- 7When we ask the LLM to generate a rationale our outline for its answer, we oftentimes see a benefit from this process, similarly to benefits observed with CoT prompting. However, it is worth noting that SoT prompting performs multiple disjoint generations (i.e., one to generate the skeleton, then one for each skeleton component), while a CoT prompt is typically used to generate output in a single pass. [View Highlight](https://readwise.io/open/713070862) ^rw713070862 
- See also: [[üëª ai highlighted]] 
