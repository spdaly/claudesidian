# OpenAI Revamps Safety Protocol

![rw-book-cover](https://readwise-assets.s3.amazonaws.com/static/images/article3.5c705a01b476.png)
<br>
>[!note]- Readwise Information
>Title:: OpenAI Revamps Safety Protocol
>Author:: [[deeplearning.ai]]
>Type:: #Readwise/category/articles
>Published-Date:: [[2024-01-10]]
>Last-Highlighted-Date:: [[2024-01-25]]
>Readwise-Link:: https://readwise.io/bookreview/37056033
>Readwise-Source:: #Readwise/source/reader
>Document-Tags:: [[artificial intelligence]] [[direct preference optimization]] [[Fine Tuning]] [[reinforcement learning human feedback]] 
>Source URL:: https://info.deeplearning.ai/ai-discovers-new-antibiotics-openai-revamps-safety-researchers-define-agi-llms-go-multimodal?ecid=ACsprvt58IsQr20HXGfCNmLOiSBIzwq4suG0-sfeAbv93-1W8tfYo15itOkz95rrUb4DvmxjFVXA&utm_campaign=The%20Batch&utm_medium=email&_hsmi=289523258&utm_content=289523258&utm_source=hs_email
--- 

## Linked Notes
```dataview
LIST
FROM [[OpenAI Revamps Safety Protocol by deeplearning.ai Highlights]]
```

---

## Highlights
- [Direct Preference Optimization](https://arxiv.org/abs/2305.18290?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=289523258&_hsenc=p2ANqtz--Q2gEbtcHqsq3C-lCw59kUqSJoVA98mIVykKi9cNDScALhJ20CLI3du4ukmrEczrsypR91WsCwdnc8-o0s1GsRvv7tKdNFCS3s26wj9fXCDD0PbcI) (DPO) [View Highlight](https://readwise.io/open/666824255) ^rw666824255
- reinforcement learning from human feedback [View Highlight](https://readwise.io/open/666824103) ^rw666824103
- This is a relatively complex algorithm. It needs to separately represent a reward function and an LLM. Also, the final, reinforcement learning step is well known to be finicky to the choice of hyperparameters. [View Highlight](https://readwise.io/open/666824123) ^rw666824123
- DPO dramatically simplifies the whole thing. Rather than needing separate transformer networks to represent a reward function and an LLM, the authors show how, given an LLM, you can figure out the reward function (plus regularization term) that that LLM is best at maximizing. [View Highlight](https://readwise.io/open/666824313) ^rw666824313
- That we can replace such fundamental building blocks of LLMs is a sign that the field is still new and much innovation lies ahead. [View Highlight](https://readwise.io/open/666824413) ^rw666824413
- Researchers at Google led by Meredith Ringel Morris [propose](https://arxiv.org/abs/2311.02462?utm_campaign=The%20Batch&utm_source=hs_email&utm_medium=email&utm_content=289523258&_hsenc=p2ANqtz--Q2gEbtcHqsq3C-lCw59kUqSJoVA98mIVykKi9cNDScALhJ20CLI3du4ukmrEczrsypR91WsCwdnc8-o0s1GsRvv7tKdNFCS3s26wj9fXCDD0PbcI) a taxonomy of AI systems according to their degree of generality and ability to perform cognitive tasks. They consider today’s large multimodal models to be “emerging AGI.” [View Highlight](https://readwise.io/open/666824682) ^rw666824682
- The taxonomy categorizes systems as possessing narrow skills (not AGI) or general capabilities (AGI). It divides both narrow and general systems into five levels of performance beyond calculator-grade Level 0. It also includes a metric for degree of autonomy. [View Highlight](https://readwise.io/open/666824699) ^rw666824699
- AGI is one of the tech world’s hottest buzzwords, yet it has had no clear definition, and various organizations propose different definitions. [View Highlight](https://readwise.io/open/666824711) ^rw666824711
