# AI Shockwave CPE Workshop - 2 Hour Presentation

**Workshop Title**: Navigating the Enterprise AI Shockwave: From Pilot Purgatory to Production Success

**Duration**: 2 hours (120 minutes)

**Target Audience**: Business leaders, IT executives, innovation managers, consultants, and professionals involved in digital transformation

**Learning Objectives**:
1. Understand why 95% of GenAI pilots fail and the root causes of the AI shockwave
2. Master the 5% Success Formula for rapid AI revenue acceleration
3. Apply the Three-Horizon Planning Framework to real organizational scenarios
4. Develop actionable 90-day AI implementation roadmap
5. Earn 2.0 CPE credits for participating

**Delivery Format**: Interactive workshop with presentation, case studies, exercises, peer discussion, and self-assessment

---

## Workshop Agenda (120 Minutes)

### Section 1: The AI Shockwave Crisis (25 minutes)
- **Slides 1-8**: Opening, learning objectives, the crisis of execution
- **Exercise 1**: Pilot Purgatory Self-Assessment (5 min)

### Section 2: The 5% Success Formula (30 minutes)
- **Slides 9-17**: Seven factors differentiating successful AI adopters
- **Exercise 2**: Success Factor Gap Analysis (7 min)

### BREAK (10 minutes)

### Section 3: Three-Horizon Planning Framework (30 minutes)
- **Slides 18-25**: Lightning Pilots ‚Üí Scale What Works ‚Üí Agentic Transformation
- **Exercise 3**: Use Case Prioritization (10 min)

### Section 4: Implementation Roadmap (20 minutes)
- **Slides 26-30**: 90-day roadmap, metrics framework
- **Exercise 4**: Action Planning (8 min)

### Section 5: Wrap-Up & Next Steps (5 minutes)
- **Slides 31-33**: Key takeaways, resources, Q&A

---

## SECTION 1: THE AI SHOCKWAVE CRISIS

### Slide 1: Title Slide

**Title**: Navigating the Enterprise AI Shockwave
**Subtitle**: From Pilot Purgatory to Production Success

**Visual**: Lightning bolt striking a traditional organization chart, fragmenting into three modern organizational layers

**Presenter Name**: [Your Name]
**CPE Credits**: 2.0 Hours
**Date**: [Workshop Date]

**Speaker Notes**:
- Welcome participants, introduce yourself
- Explain workshop format: presentation + exercises + discussion
- Emphasize this is about **execution**, not AI technology fundamentals
- Note-taking encouraged, workbook provided

---

### Slide 2: Learning Objectives & CPE Information

**What You'll Learn Today**:

‚úÖ **Understand** why 95% of GenAI pilots fail (and how to join the 5%)
‚úÖ **Master** the Seven Success Factors for rapid AI revenue acceleration
‚úÖ **Apply** the Three-Horizon Planning Framework to your organization
‚úÖ **Develop** an actionable 90-day AI implementation roadmap
‚úÖ **Assess** your organization's AI readiness maturity

**CPE Details**:
- **Field of Study**: Information Technology / Business Management
- **Credits**: 2.0 hours
- **Level**: Intermediate
- **Prerequisites**: None (familiarity with AI concepts helpful but not required)
- **Delivery Method**: Group-Live (interactive workshop)

**Workshop Ground Rules**:
- Participate actively in exercises
- Share insights from your experience
- Respect confidentiality of peer discussions
- Ask questions anytime

**Speaker Notes**:
- Confirm attendees understand CPE requirements (attendance, participation)
- Explain sign-in sheet and evaluation form for credit documentation
- Set expectation: This is practical, not theoretical

---

### Slide 3: Poll - Current State Check-In

**Interactive Poll** (show of hands or digital polling):

**Question 1**: How many AI pilots is your organization currently running?
- 0 (just exploring)
- 1-3 pilots
- 4-10 pilots
- 10+ pilots

**Question 2**: How many AI initiatives have reached production in the past 12 months?
- 0 (none yet)
- 1-2
- 3-5
- 5+

**Question 3**: Biggest AI challenge you're facing?
- A) Getting started / picking use cases
- B) Moving pilots to production
- C) Measuring ROI / proving value
- D) Governance and risk management
- E) Organizational resistance / culture

**Visual**: Interactive poll results displayed in real-time

**Speaker Notes**:
- Use results to tailor workshop emphasis
- Acknowledge common pain points from responses
- Creates psychological safety by showing shared struggles
- Reference poll results throughout workshop

---

### Slide 4: The AI Shockwave - Evidence Base

**Visual**: Dramatic "shockwave" visualization with key statistics radiating outward

**The Crisis**:

üìà **Investment Surge**
- 2023: $2.3B enterprise AI spending
- 2024: $13.8B (6x increase in one year)
- 72% of enterprises plan broader AI adoption

‚ùå **The Failure Gap**
- **95% of GenAI pilots fail** to deliver P&L impact
- Only **5% achieve rapid revenue acceleration**
- Average time from idea to production: 6-12 months (too slow)

‚è∞ **The Time Trap**
- "Pilot purgatory": Endless POCs without production deployment
- AI capabilities advancing faster than enterprise adoption

**Sources** (footer):
- MIT Research (150 leader interviews, 350 employee surveys)
- Menlo Ventures State of GenAI 2024
- McKinsey State of AI Early 2024

**Speaker Notes**:
- Let statistics sink in: "95% failure rate is staggering"
- This isn't a technology problem‚Äîit's an execution problem
- The 6x spending increase creates urgency + pressure
- Most organizations know they need AI but don't know how to succeed
- Transition: "Let's understand WHY this is happening"

---

### Slide 5: The Root Cause - Time Compression Challenge

**Visual**: Side-by-side timeline comparison

**LEFT SIDE: Traditional Innovation Timelines**
```
Discovery (6 weeks) ‚Üí Scoping (4 weeks) ‚Üí Business Case (8 weeks) ‚Üí
Development (16 weeks) ‚Üí Testing (6 weeks) ‚Üí Launch (4 weeks)
= 44 weeks (10-11 months)
```

**RIGHT SIDE: AI Velocity Requirements**
```
Rapid Discovery (3 days) ‚Üí Scoping + Business Case (4 days) ‚Üí
MVP Development (2 weeks) ‚Üí Testing in Production (2 weeks) ‚Üí
Scale Decision (3 days)
= 6 WEEKS TOTAL
```

**Core Insight** (callout box):
> The AI shockwave is fundamentally a **TIME COMPRESSION CHALLENGE**, not a technology challenge. Organizations applying 10-month timelines to initiatives requiring 6-week velocity end up in "pilot purgatory."

**Visual**: Hourglass graphic showing sand falling but getting stuck halfway (representing stuck pilots)

**Speaker Notes**:
- This is the #1 insight: It's about SPEED, not sophistication
- Traditional Stage-Gate designed for physical products with high tooling costs
- AI has low technical risk but high adoption risk‚Äîgovernance must adapt
- 6-week rule comes from research on successful AI adopters
- Quote: "The thing that kills innovation in big companies is a time frame problem" - Paul Baier

---

### Slide 6: What Is "Pilot Purgatory"?

**Visual**: Organizational graveyard with tombstones labeled with failed pilot names

**Pilot Purgatory Defined**:
Organizational dysfunction where AI initiatives remain in endless testing/POC phase without reaching production deployment.

**Symptoms** (with icons):
üîÑ "We're still testing..."
üìä "We need more data before deciding..."
‚è≥ "Let's wait for the perfect use case..."
üé≠ "Innovation theater": Lots of activity, no results
üíº Multiple POCs running, none reaching real users
üìà Budget spent on pilots, zero production revenue

**The Hidden Cost**:
- Average pilot costs $50K-$200K
- If 95% fail, every production success costs $1M-$4M in wasted pilots
- Opportunity cost: Competitors moving faster
- Morale cost: Team burnout from failed initiatives

**Real Example** (anonymized):
"Fortune 500 company ran 23 GenAI pilots over 18 months. Zero reached production. Innovation team disbanded."

**Speaker Notes**:
- Ask audience: "Who's seen this in their organization?" (show of hands)
- Pilot purgatory is a symptom, not the disease
- Root causes: Risk aversion, analysis paralysis, governance theater
- Good news: It's fixable with the right approach

---

### Slide 7: The 5% vs. The 95%

**Visual**: Two-column comparison table with contrasting behaviors

| **The 95% (Pilot Purgatory)** | **The 5% (Rapid Revenue Acceleration)** |
|-------------------------------|------------------------------------------|
| Generic "AI transformation" goals | Specific use cases with clear ROI targets |
| 6-12 month development timelines | 6-week Lightning Pilot cycles |
| Overlay AI on existing broken processes | Redesign workflows before adding AI |
| Same governance for all initiatives | Risk-proportionate, speed-enabling governance |
| Reactive risk management (firefighting) | Proactive risk mitigation from day 1 |
| Punish failures, hide experiments | Celebrate intelligent failures, share learnings |
| Siloed teams (IT vs. Business) | Cross-functional collaboration from start |
| Waiting for "perfect" before production | Ship to production fast, iterate in production |
| Vanity metrics (# of pilots) | Business impact metrics (revenue, cost reduction) |
| One pilot at a time | Portfolio approach: Run 5 pilots, expect 2-3 to succeed |

**Callout Box**:
"The 5% don't have better AI technology. They have better **execution discipline**."

**Speaker Notes**:
- These differences aren't obvious‚Äîthat's why most fail
- 5% have internalized that speed is the #1 success factor
- They've adapted governance and culture for AI velocity
- Transition: "Let's do a self-assessment exercise"

---

### Slide 8: EXERCISE 1 - Pilot Purgatory Self-Assessment

**Instructions**:

Score your organization on each dimension (1-5 scale):
- 1 = Strongly describes the 95% (left column)
- 5 = Strongly describes the 5% (right column)

**Dimensions** (from previous slide):
1. Goal clarity (generic transformation vs. specific use cases)
2. Timeline discipline (6-12 months vs. 6 weeks)
3. Workflow redesign (overlay vs. redesign-first)
4. Governance approach (same-for-all vs. risk-proportionate)
5. Risk management (reactive vs. proactive)
6. Failure culture (punish vs. celebrate intelligent failures)
7. Team structure (siloed vs. cross-functional)
8. Production mindset (wait for perfect vs. ship fast, iterate)
9. Metrics focus (vanity vs. business impact)
10. Portfolio approach (one at a time vs. parallel pilots)

**Total Score**: _____ / 50

**Scoring Guide**:
- 40-50: Well-positioned for AI success
- 30-39: Some strengths, significant gaps remain
- 20-29: High risk of pilot purgatory
- 10-19: Urgent need for transformation

**Time**: 5 minutes (silent individual work)

**Speaker Notes**:
- This is for personal reflection, not judgment
- No need to share scores publicly (unless they want to)
- After exercise: "By show of hands, how many scored below 30?" (gauge room)
- Transition: "Now let's learn the 5% Success Formula"

---

## SECTION 2: THE 5% SUCCESS FORMULA

### Slide 9: Section Intro - The 5% Success Formula

**Visual**: Formula equation graphic

```
5% Success = Tactical Clarity + Compressed Cycles + Workflow Redesign +
             Speed-Enabling Governance + Proactive Risk +
             Continuous Learning + Cross-Functional Collaboration
```

**Overview**:
Research on successful AI adopters identified **7 common factors**. Organizations that master all 7 achieve rapid revenue acceleration. Missing even one significantly reduces success probability.

**What We'll Cover** (numbered list):
1. Tactical Clarity Over Strategic Ambiguity
2. Compressed Innovation Cycles (6 Weeks Not 6 Months)
3. Workflow Redesign Before AI Deployment
4. Speed-Enabling Governance
5. Proactive Risk Management
6. Continuous Learning Culture
7. Cross-Functional Collaboration

**Time for This Section**: 30 minutes

**Speaker Notes**:
- These aren't theoretical‚Äîthey're evidence-based from MIT/McKinsey research
- Each factor alone helps; all 7 together create breakthrough results
- We'll cover each briefly, then do gap analysis exercise

---

### Slide 10: Success Factor #1 - Tactical Clarity

**Visual**: Bullseye target with specific use case in center, vague "AI transformation" in outer rings

**What the 5% Do**:
‚úÖ Pick **specific use cases** with measurable success criteria
‚úÖ Focus on proven high-ROI categories:
   - Code copilots (51% adoption, 30-50% productivity gain)
   - Support chatbots (31% adoption, 40% faster resolution)
   - Enterprise search (28% adoption, 50% time savings)
‚úÖ Set concrete targets: "Reduce support ticket resolution time by 40%" (not "improve customer service")

**What the 95% Do**:
‚ùå Chase generic "AI transformation" without specifics
‚ùå Endless strategic planning meetings
‚ùå Analysis paralysis waiting for "perfect" use case
‚ùå Try to boil the ocean

**Real Example - The 5% Way**:
"**Goal**: Reduce software development cycle time by 35% using GitHub Copilot for 50-person dev team. **Success metric**: Ship 2-week sprints in 10 days. **Timeline**: 6-week pilot with 10 developers, then scale to full team."

**Key Insight**:
Start narrow and deep, not broad and shallow. Expansion comes from success, not big-bang planning.

**Speaker Notes**:
- Use case adoption rates show what's actually working
- Tactical clarity doesn't mean thinking small‚Äîit means being specific
- You can do transformational AI, but start with one specific use case
- Ask: "Who has a use case this specific?" (likely few hands)

---

### Slide 11: Success Factor #2 - Compressed Cycles

**Visual**: Speedometer graphic showing traditional innovation at 10-12 months, AI velocity at 6 weeks

**The 6-Week Rule**:
> "The thing that kills innovation in big companies is a time frame problem. The solution is 6 weeks from idea to production." - Paul Baier, GAI Insights

**What the 5% Do**:
‚úÖ Implement 6-week Lightning Pilot cycles
‚úÖ Make go/kill decisions weekly (not quarterly)
‚úÖ Ship to production fast, iterate in production
‚úÖ Compress gates: 3 days discovery, 2 weeks MVP, 2 weeks testing, 3 days scale decision

**What the 95% Do**:
‚ùå Apply traditional 6-12 month development timelines
‚ùå Endless refinement before production
‚ùå "Wait until it's perfect" mentality
‚ùå Quarterly governance reviews (too slow)

**Time Compression Techniques**:
1. **Parallel gates**: Run discovery + scoping simultaneously
2. **Fast-track approvals**: <$100K pilots approved in 48 hours
3. **Production testing**: Deploy to limited users (10-50) immediately
4. **Binary decisions**: Go/kill at week 6 (no "let's extend the pilot")

**ROI of Speed**:
- 6-week cycle = 8 learning cycles per year
- 6-month cycle = 2 learning cycles per year
- **4x more opportunities to find what works**

**Speaker Notes**:
- Speed is THE differentiator between 5% and 95%
- This feels uncomfortable‚Äîthat's normal
- Speed requires trusting imperfect data over perfect plans
- Ask: "What's your current average pilot timeline?" (likely 6+ months)

---

### Slide 12: Success Factor #3 - Workflow Redesign

**Visual**: Before/after process diagram

**BEFORE (The 95%)**: Broken Process + AI Overlay = Amplified Problems
```
[Slow manual approval process] + [AI chatbot] = [Fast wrong answers]
```

**AFTER (The 5%)**: Redesigned Process + AI Enablement = Transformation
```
[Streamlined approval workflow] + [AI copilot] = [10x faster, accurate decisions]
```

**What the 5% Do**:
‚úÖ Redesign business processes **before** overlaying AI
‚úÖ Ask "How should this work with AI?" not "Where can we add AI?"
‚úÖ Involve end users in workflow redesign from day 1
‚úÖ Fix broken processes first, then accelerate with AI

**What the 95% Do**:
‚ùå Overlay AI on broken existing processes
‚ùå Technology-first approach ("We have AI, now where do we use it?")
‚ùå Expect AI to magically fix process problems
‚ùå Skip the hard work of process improvement

**Real Example - Customer Support**:
- **95% approach**: Add chatbot to existing 47-step ticket routing process ‚Üí Customers frustrated with bot, agents overwhelmed
- **5% approach**: Redesign to 3-tier system (bot handles tier 1, humans tier 2/3) ‚Üí 60% tickets resolved by bot, agent satisfaction improves

**Key Principle**:
"AI amplifies existing workflows. If your process is broken, AI makes it fail faster."

**Speaker Notes**:
- This is the hardest factor‚Äîrequires organizational courage
- Many organizations avoid process redesign and wonder why AI fails
- Workflow redesign is change management, not technology work
- Best AI initiatives are 70% process redesign, 30% technology

---

### Slide 13: Success Factor #4 - Speed-Enabling Governance

**Visual**: Two governance models side-by-side

**LEFT: Governance Theater (The 95%)**
- Same approval process for $50K pilot as $5M investment
- Requires same documentation as traditional IT projects
- Reviews happen quarterly (way too slow)
- Gates are rubber stamps (no real decision-making)
- Risk aversion kills experimentation

**RIGHT: Risk-Proportionate Governance (The 5%)**
- **Low-risk** (<$100K, no customer data, limited scope): Fast-track approval in 48 hours
- **Medium-risk** (customer-facing, moderate investment): 2-week approval with business case
- **High-risk** (regulated data, >$1M, transformational): Traditional Stage-Gate governance
- Weekly go/kill reviews for pilots
- Governance that accelerates decisions, not gates them

**Governance Principles**:
1. **Match rigor to risk**: AI has lower technical risk than traditional IT
2. **Enable speed**: Remove unnecessary gates, compress approval cycles
3. **Real decision-making**: Healthy kill rates (50-70% of ideas killed early)
4. **Proactive, not reactive**: Set guardrails before pilot starts

**The Governance Paradox**:
"The 5% have MORE governance oversight, but it's designed to enable speed, not constrain it."

**Speaker Notes**:
- Most organizations have too much governance or too little (both fail)
- Risk-proportionate governance requires trust + discipline
- Fast-track approvals work because guardrails are clear upfront
- Ask: "How long does a $75K pilot approval take in your organization?" (likely weeks/months)

---

### Slide 14: Success Factor #5 - Proactive Risk Management

**Visual**: Two-column comparison

**Reactive Risk Management (The 95%)**
‚ùå Build first, address risks later
‚ùå "Move fast and break things" without safeguards
‚ùå Firefighting when risks materialize
‚ùå OR: Risk paralysis that prevents any experimentation
‚ùå Compliance as afterthought

**Proactive Risk Management (The 5%)**
‚úÖ Address AI risks from day 1: Bias, hallucination, privacy, security
‚úÖ Build guardrails into pilots from the start
‚úÖ Elevate ethics oversight while maintaining speed
‚úÖ Test failure modes before production
‚úÖ Compliance integrated into design

**Key AI Risks to Address Proactively**:
1. **Hallucination**: AI making up facts (mitigation: citations, human-in-loop for critical decisions)
2. **Bias**: Training data biases (mitigation: diverse testing, RLHF, audit trails)
3. **Privacy**: Sensitive data exposure (mitigation: data minimization, access controls)
4. **Security**: Prompt injection attacks (mitigation: input validation, sandboxing)
5. **Compliance**: Regulatory requirements (mitigation: early legal review, documentation)

**The Proactive Risk Advantage**:
Addressing risks early enables sustainable speed. Reactive approaches create stop-the-world incidents that kill momentum.

**Real Example**:
"Healthcare AI chatbot pilot: 5% approach built patient data privacy controls before deployment. 95% approach launched publicly, exposed PHI, pilot shut down for 6 months during investigation."

**Speaker Notes**:
- Proactive risk management isn't slower‚Äîit's faster long-term
- The 5% don't skip risks, they architect for them upfront
- This requires cross-functional teams (legal, compliance, security from day 1)
- Risk management is competitive advantage, not compliance burden

---

### Slide 15: Success Factor #6 - Continuous Learning Culture

**Visual**: Learning cycle diagram with "intelligent failures" celebrated

**What the 5% Do**:
‚úÖ Celebrate intelligent failures (fast learning from experiments)
‚úÖ Document lessons from killed pilots (failure = data)
‚úÖ Share learnings across organization (avoid repeating mistakes)
‚úÖ Blameless post-mortems
‚úÖ Portfolio mindset: Run 5 pilots, expect 2-3 to succeed (3 failures are learnings, not waste)

**What the 95% Do**:
‚ùå Punish failures, reward only successes
‚ùå Hide failed experiments (to protect career)
‚ùå Repeat same mistakes across teams
‚ùå "Failure is not an option" culture
‚ùå Each team reinvents the wheel

**The Math of Learning**:
- **95% pilot failure rate** means most experiments will fail
- Organizations that learn from failures improve success rate to 40-60% over time
- 5 pilots with 40% success rate = 2 production wins
- **Learning compounds**: Lessons from Pilot 1 improve Pilot 6 success probability

**How to Build Learning Culture**:
1. **Failure templates**: Standardize what to document (hypothesis, test, result, lesson)
2. **Learning reviews**: Monthly showcase of both wins AND learnings from killed pilots
3. **Playbooks**: Extract patterns from multiple pilots into reusable playbooks
4. **Incentives**: Reward teams for generating valuable learnings, not just successes

**Quote**:
> "We celebrate failure. If you're not failing, you're not trying hard enough." - WD-40 CEO Gary Ridge

**Speaker Notes**:
- This is cultural, not technical‚Äîhardest to change
- Ask: "How does your org treat failed pilots?" (gauge culture)
- Learning culture doesn't mean accepting sloppy work‚Äîit means valuing data
- The 5% extract value from every pilot (success = revenue, failure = learning)

---

### Slide 16: Success Factor #7 - Cross-Functional Collaboration

**Visual**: Venn diagram showing Business + IT + Data Science overlapping

**What the 5% Do**:
‚úÖ Business + IT + Data Science working together from day 1
‚úÖ Co-located teams with shared goals
‚úÖ Product owners empowered to make decisions
‚úÖ "Two-pizza teams" (small, autonomous, cross-functional)
‚úÖ Business owns outcomes, IT enables technology, Data Science ensures quality

**What the 95% Do**:
‚ùå Siloed teams ("IT will build it, then hand to business")
‚ùå Throw-over-the-wall handoffs
‚ùå No clear ownership or accountability
‚ùå Business defines requirements ‚Üí IT builds ‚Üí Business rejects ("not what we meant")
‚ùå Waterfall approach in an AI world

**The Collaboration Imperative**:
AI success requires tight collaboration because:
- **Business** understands the problem and ROI
- **IT** understands architecture and integration
- **Data Science** understands model behavior and limitations
- All three must work together to redesign workflows

**Team Structure - The 5% Way**:
```
Product Owner (Business): Defines success criteria, prioritizes features
Tech Lead (IT): Ensures scalability, security, integration
ML Engineer (Data Science): Builds and tunes models
UX Designer: Ensures usability and adoption
```
‚Üí Team sits together, daily standups, shared OKRs

**Collaboration Anti-Patterns**:
- "IT will figure it out" (business abdication)
- "Business doesn't understand AI" (IT arrogance)
- "We'll build the perfect model, then find a use case" (data science detachment)

**Speaker Notes**:
- Silos are the #1 organizational blocker to AI success
- Cross-functional doesn't mean everyone does everything‚Äîit means tight collaboration
- Ask: "How many have co-located cross-functional AI teams?" (likely few)
- This requires organizational redesign, not just process change

---

### Slide 17: EXERCISE 2 - Success Factor Gap Analysis

**Instructions**:

For each of the 7 Success Factors, rate your organization:

**Gap Size** (1-5 scale):
- 1 = No gap (we're doing this well)
- 5 = Huge gap (we're not doing this at all)

**Impact** (1-5 scale):
- 1 = Low priority (won't move the needle much)
- 5 = High priority (critical to our success)

**Priority Score** = Gap Size √ó Impact (1-25)

| Success Factor | Gap Size (1-5) | Impact (1-5) | Priority Score |
|----------------|----------------|--------------|----------------|
| 1. Tactical Clarity | ___ | ___ | ___ |
| 2. Compressed Cycles | ___ | ___ | ___ |
| 3. Workflow Redesign | ___ | ___ | ___ |
| 4. Speed-Enabling Governance | ___ | ___ | ___ |
| 5. Proactive Risk Mgmt | ___ | ___ | ___ |
| 6. Continuous Learning | ___ | ___ | ___ |
| 7. Cross-Functional Collab | ___ | ___ | ___ |

**Action**: Circle your top 2-3 priority scores (highest numbers)

**Pair Discussion** (5 min):
- Share your top priority with a neighbor
- Discuss one action to close that gap

**Time**: 7 minutes total (3 min individual, 4 min discussion)

**Speaker Notes**:
- This helps prioritize where to focus improvement efforts
- Highest priority scores = biggest impact if addressed
- After exercise: "Who wants to share their #1 priority?" (2-3 volunteers)
- Transition: "Now let's take a 10-minute break"

---

## BREAK (10 Minutes)

**Slide**: Break slide with timer

**Visual**: Coffee cup icon

**Text**: "10-Minute Break - Return at [TIME]"

**Networking Prompt**:
"During the break, discuss with a colleague: What's one AI pilot your organization is running right now, and what stage is it in?"

**Speaker Notes**:
- Restart promptly at break end
- Use break to gauge room energy, adjust pacing if needed

---

## SECTION 3: THREE-HORIZON PLANNING FRAMEWORK

### Slide 18: Section Intro - Three-Horizon Framework

**Visual**: Three ascending horizons timeline

```
Horizon 1           Horizon 2              Horizon 3
Lightning Pilots    Scale What Works       Agentic Transformation
[0-6 weeks]        [6 weeks - 6 months]   [6-24 months]
```

**The Planning Challenge**:
How do you balance immediate tactical AI wins (Horizon 1) with long-term strategic transformation (Horizon 3)?

**The Three-Horizon Answer**:
- **Horizon 1**: Prove value quickly, kill failures faster (Lightning Pilots)
- **Horizon 2**: Industrialize successful pilots into repeatable capabilities (Scale What Works)
- **Horizon 3**: Transform business processes with AI-native workflows (Agentic Transformation)

**Portfolio Balance**:
Run all three horizons **simultaneously**:
- 70% effort on Horizon 1 (proven quick wins)
- 20% effort on Horizon 2 (scaling successes)
- 10% effort on Horizon 3 (transformational bets)

**Time for This Section**: 30 minutes

**Speaker Notes**:
- Three horizons prevent "pilot purgatory" by creating clear graduation path
- H1 ‚Üí H2 ‚Üí H3 is a funnel, not stages
- Successful H1 pilots graduate to H2, best H2 initiatives become H3 transformations
- This framework integrates with existing innovation models (Stage-Gate, Lean Startup, Agile)

---

### Slide 19: Horizon 1 - Lightning Pilots (0-6 Weeks)

**Visual**: Lightning bolt icon with 6-week timeline

**Objective**: Prove value quickly, kill failures faster

**Timeline**: 6 weeks from idea to production deployment

**The Lightning Pilot Process**:

**Week 1-2: Rapid Assessment & Use Case Selection**
- Identify 5-10 candidate use cases
- Score on: Business impact, Technical feasibility, Workflow redesign potential
- Select top 5 for simultaneous pilots

**Week 3-4: MVP Development & Testing**
- Build minimum viable solution (not perfect product)
- Test with limited user group (10-50 users)
- Focus on workflow integration, not technology features

**Week 5-6: Production Deployment & Measurement**
- Deploy to production environment (limited scope)
- Measure business impact metrics
- Make binary go/kill decision

**Success Metrics**:
- 40-60% of pilots reach production deployment
- Measurable business impact (time saved, quality improved, revenue increased)
- User adoption >50% within pilot group

**Portfolio Approach**:
Run 5 pilots simultaneously, expect 2-3 to succeed. Failed pilots generate learning, not wasted investment.

**Governance**:
Weekly go/kill reviews with binary decisions (no "let's wait and see")

**Key Capability Required**:
Workflow redesign expertise (not just technology deployment)

**Speaker Notes**:
- This is radically different from traditional 6-12 month pilots
- 6 weeks forces discipline‚Äîno time for scope creep
- Binary decision at Week 6: Scale it or kill it (no perpetual pilots)
- Ask: "What would need to change to run 6-week pilots?" (governance, procurement, staffing)

---

### Slide 20: Horizon 1 Example - Lightning Pilot in Action

**Real Example**: Code Copilot for Internal Development Team

**Week 1-2: Rapid Assessment**
- **Problem**: Software development cycle time too long (2-week sprints take 12 days)
- **Proposed solution**: GitHub Copilot for 10-person dev team
- **Success metric**: Reduce sprint completion time by 30% (from 12 days to 8-9 days)
- **Business case**: $15K pilot cost, potential $200K annual savings in faster delivery
- **Approval**: Fast-track (low risk, internal users, proven technology)

**Week 3-4: MVP Development**
- Deploy GitHub Copilot to 10 developers
- Train team on effective prompting
- Measure: Lines of code written, PR velocity, developer satisfaction
- Workflow redesign: Pair programming with AI, AI-assisted code review

**Week 5-6: Production & Measurement**
- Full sprint cycle with Copilot
- Results: Sprint completion dropped from 12 days to 9 days (25% improvement)
- User adoption: 90% of developers use Copilot daily
- Unexpected benefit: Junior developers onboard 40% faster

**Week 6 Decision: SCALE**
- ROI proven: 25% cycle time reduction
- High user satisfaction
- Decision: Expand to all 50 developers (Horizon 2)
- Investment: $50K annual licenses, expected $600K value from faster delivery

**Lessons Learned**:
- Workflow integration (pair programming) was key to success
- Training critical‚Äîinitial adoption low until we showed effective use
- Metrics must be business outcomes (cycle time), not vanity metrics (lines of code)

**Speaker Notes**:
- This example shows all 7 success factors in action
- Notice: Business outcome (cycle time), not technology outcome (AI accuracy)
- 6-week timeline created urgency and focus
- Binary decision at Week 6 prevented pilot purgatory

---

### Slide 21: Horizon 2 - Scale What Works (6 Weeks - 6 Months)

**Visual**: Scaling graph showing 10x user adoption

**Objective**: Industrialize successful pilots into repeatable capabilities

**Timeline**: 6 weeks to 6 months from pilot success to enterprise-scale deployment

**The Scaling Process**:

**Months 1-3: Scaling Successful Pilots**
- Expand user base 10x (from 50 to 500+ users)
- Standardize deployment process
- Document success patterns
- Build change management program
- Address scaling challenges (infrastructure, support, training)

**Months 3-6: Platform & Capability Building**
- Establish AI Center of Excellence
- Create reusable AI components
- Develop internal training programs
- Build guardrails and governance standards
- Measure ROI and business impact

**Success Metrics**:
- 10x user adoption vs. pilot phase
- Documented ROI (financial impact)
- Repeatable deployment playbook
- <10% rollback/failure rate during scaling

**Portfolio Approach**:
70/20/10 balance
- 70%: Scaling proven use cases
- 20%: New Lightning Pilot experiments
- 10%: Exploratory bets on emerging capabilities

**Governance**:
Bi-weekly strategic reviews that maintain speed while elevating oversight

**Key Capability Required**:
Change management and training at scale

**Speaker Notes**:
- Scaling is where most organizations fail‚Äîdifferent skills than piloting
- This is about operationalizing, not just expanding
- 10x user growth reveals issues invisible in 50-user pilot
- Change management becomes critical (pilot had volunteers, scaling has everyone)

---

### Slide 22: Horizon 3 - Agentic Transformation (6-24 Months)

**Visual**: Autonomous agent network diagram

**Objective**: Move from human-in-loop copilots to autonomous agents

**Timeline**: 6-24 months for transformational business process redesign

**The Agentic AI Vision**:
AI agents that autonomously execute complex multi-step tasks without human intervention (vs. copilots that assist humans)

**The Transformation Process**:

**Months 6-12: Agentic AI Foundation**
- Pilot Large Action Models (LAMs)
- Design multi-agent systems
- Identify processes suitable for full automation
- Build AI ethics and oversight frameworks

**Months 12-24: AI-Native Business Processes**
- Redesign end-to-end processes around AI agents
- Deploy autonomous agents in production
- Measure business transformation impact
- Develop AI-powered products/services

**Success Metrics**:
- Business processes that are AI-native (redesigned, not retrofitted)
- Autonomous agent operation with <5% escalation to humans
- Measurable business model transformation
- AI-enabled new revenue streams

**Portfolio Approach**:
Strategic bets on transformational innovations
- Focus on 2-3 high-impact business processes
- Accept longer timelines and higher risk
- Use traditional Stage-Gate governance for large investments

**Governance**:
Traditional Stage-Gate for high-investment transformational initiatives

**Key Capability Required**:
Business process engineering and AI ethics frameworks

**Speaker Notes**:
- Horizon 3 is aspirational for most organizations (still in H1/H2)
- Don't skip H1/H2 to jump to H3‚Äîyou'll fail
- Agentic AI is where the industry is heading (Gartner predicts by 2027)
- Example: Customer service agent that autonomously resolves 80% of issues end-to-end

---

### Slide 23: Three Horizons Integration

**Visual**: Waterfall diagram showing how pilots flow through horizons

**The Funnel**:
```
Horizon 1: Run 20 Lightning Pilots per year
    ‚Üì (40-60% success rate)
Horizon 2: Scale 8-12 successful pilots
    ‚Üì (50% reach enterprise scale)
Horizon 3: Transform 4-6 business processes into AI-native workflows
```

**Portfolio Management**:

**Active Portfolio at Any Time**:
- **15-20 H1 pilots** running simultaneously (6-week cycles)
- **5-8 H2 scaling initiatives** (2-6 month timelines)
- **2-3 H3 transformation programs** (12-24 month investments)

**Resource Allocation**:
- 70% budget/effort on H1 (proven quick wins)
- 20% budget/effort on H2 (scaling successes)
- 10% budget/effort on H3 (transformational bets)

**Graduation Criteria**:
- **H1 ‚Üí H2**: Production deployment + measurable ROI + user adoption >50%
- **H2 ‚Üí H3**: Enterprise-scale adoption + documented ROI + strategic importance

**Review Cadence**:
- **H1**: Weekly go/kill decisions for each pilot
- **H2**: Bi-weekly scaling progress reviews
- **H3**: Monthly strategic program reviews

**The Virtuous Cycle**:
H1 success funds H2 scaling, H2 success funds H3 transformation, H3 creates competitive moats

**Speaker Notes**:
- This creates a healthy innovation pipeline (not pilot purgatory)
- Clear graduation criteria prevent ambiguity
- Portfolio balance prevents over-indexing on quick wins or moonshots
- Ask: "Where is most of your AI effort today?" (likely all H1 or scattered)

---

### Slide 24: Use Case Prioritization Framework

**Visual**: Scoring matrix with example use cases plotted

**How to Select Lightning Pilot Use Cases**:

**Scoring Dimensions** (1-5 scale each):

1. **Business Impact**: >50% improvement in key metric = 5 points
2. **Technical Feasibility**: Off-the-shelf solution with minimal customization = 5 points
3. **Workflow Redesign Potential**: Enables complete process transformation = 5 points
4. **Risk Level**: Internal use, no customer data, limited scope = 5 points (low risk)
5. **6-Week Feasibility**: Can deploy to production in 4 weeks = 5 points

**Priority Score Formula**:
```
(Business Impact √ó 3) + (Technical Feasibility √ó 2) + (Workflow Redesign √ó 2) - (Risk √ó 1) + (6-Week Feasibility √ó 2)
```

**Target**: Select use cases scoring >40 for Lightning Pilots

**Example Scoring**:

| Use Case | Biz Impact | Tech Feas | Workflow | Risk | 6-Week | Score |
|----------|------------|-----------|----------|------|--------|-------|
| Code Copilot | 5 | 5 | 4 | 1 | 5 | **48** ‚úÖ |
| Support Chatbot | 4 | 4 | 5 | 3 | 4 | **43** ‚úÖ |
| Enterprise Search | 4 | 5 | 3 | 2 | 5 | **45** ‚úÖ |
| Sales Forecasting AI | 5 | 2 | 2 | 4 | 2 | **31** ‚ùå |
| AI-Powered Product Design | 5 | 3 | 5 | 3 | 3 | **41** ‚úÖ |

**Prioritization Insights**:
- High-scoring use cases = High probability of H1 ‚Üí H2 graduation
- Low scores don't mean never‚Äîthey mean "not ready for Lightning Pilot yet"
- Use for quarterly planning: Identify top 20 use cases, launch 5 per quarter

**Speaker Notes**:
- This scoring prevents "shiny object syndrome" and analysis paralysis
- Weights reflect what predicts success (business impact most important)
- Risk is subtracted (higher risk lowers score)
- Use this in the upcoming exercise

---

### Slide 25: EXERCISE 3 - Use Case Prioritization

**Instructions**:

**Step 1** (3 min): Brainstorm 3-5 potential AI use cases for your organization

Examples to spark ideas:
- Code copilots for developers
- Support chatbots for customer service
- Enterprise search / knowledge management
- Data analysis / business intelligence
- Content generation (marketing, documentation)
- Sales forecasting
- Fraud detection
- Recruitment screening

**Step 2** (5 min): Score your top 2 use cases using the prioritization framework

| Use Case | Biz Impact (√ó3) | Tech Feas (√ó2) | Workflow (√ó2) | Risk (-1) | 6-Week (√ó2) | Score |
|----------|-----------------|----------------|---------------|-----------|-------------|-------|
| 1. _________ | ___ | ___ | ___ | ___ | ___ | ___ |
| 2. _________ | ___ | ___ | ___ | ___ | ___ | ___ |

**Step 3** (2 min): Identify your highest-scoring use case for potential Lightning Pilot

**Workbook Reference**: Full scoring rubric and definitions in workbook pages 12-13

**Time**: 10 minutes total

**Speaker Notes**:
- Circulate and help with scoring questions
- After exercise: "Who has a use case scoring >40?" (show of hands)
- "Who wants to share their highest-scoring use case?" (2-3 volunteers)
- Transition: "Now let's talk about how to implement this"

---

## SECTION 4: IMPLEMENTATION ROADMAP

### Slide 26: Your First 90 Days - Implementation Roadmap

**Visual**: 90-day Gantt chart

**Weeks 1-2: Assessment & Mobilization**

‚úÖ **Conduct Organizational Readiness Assessment**
- Use 4-level maturity model (we'll cover next slide)
- Identify current state, gaps, and target state
- **Deliverable**: 1-page readiness assessment

‚úÖ **Identify 5-10 Lightning Pilot Candidates**
- Use prioritization framework from Exercise 3
- Score all candidates, select top 5
- **Deliverable**: Prioritized pilot list with business sponsors

‚úÖ **Establish AI Governance Framework**
- Define risk-proportionate approval thresholds
- Create fast-track process for <$100K pilots
- **Deliverable**: 1-page governance framework

‚úÖ **Secure Executive Sponsorship**
- Present business case for AI acceleration
- Get commitment to 6-week cycle discipline
- **Deliverable**: Executive sign-off and budget

---

**Weeks 3-8: Lightning Pilot Sprint (6-Week Cycle)**

**Week 3-4**: Rapid Discovery & MVP Development
- Kick off all 5 pilots simultaneously
- Focus on workflow redesign first
- Build MVPs (not perfect products)

**Week 5-6**: Testing & Early Production
- Deploy to limited user groups (10-50 users each)
- Gather usage data and feedback
- Measure business impact metrics

**Week 7**: Go/Kill Decisions
- Review results for all 5 pilots
- Make binary decisions: Scale or Kill
- Document learnings from killed pilots

**Week 8**: Production Deployment
- Deploy successful pilots to production (limited scope)
- Set up monitoring and success tracking
- Plan scaling approach for Horizon 2

**Weekly Governance**:
- Monday: Progress updates
- Wednesday: Remove blockers
- Friday: Review metrics, adjust

**Target**: 2-3 of 5 pilots reach production

---

**Weeks 9-12: Scale Decision & Learning**

**Week 9-10**: Results Analysis
- Document what worked and what failed
- Identify success patterns and failure modes
- Calculate ROI for successful pilots

**Week 11**: Strategic Planning
- Decide which pilots to scale (Horizon 2)
- Plan next round of Lightning Pilots
- Adjust governance based on learnings

**Week 12**: Capability Building
- Share learnings across organization
- Refine 6-week cycle playbook
- Plan investments in platform capabilities

**Deliverables**:
- Lightning Pilot results report
- Scaling roadmap for successful pilots
- Refined playbook for next cycle
- Organizational learning summary

**Speaker Notes**:
- This roadmap is prescriptive‚Äîadapt to your context
- 90 days gets you through one full Lightning Pilot cycle + planning for scale
- By Day 90, you'll have 2-3 production AI deployments and clear playbook
- Ask: "What's the biggest blocker to starting Week 1 in your organization?"

---

### Slide 27: Organizational Readiness Maturity Model

**Visual**: 4-level maturity ladder

**Level 1: Reactive (Pilot Purgatory)**

**Symptoms**:
- Endless POCs with no production deployments
- "Waiting for perfect use case" mentality
- AI treated as IT project, not business transformation
- No clear AI strategy or ownership

**AI Maturity**: Exploring

**Next Step**:
Pick 1 tactical use case with clear business sponsor. Commit to 6-week production deployment.

**Example Pilot**: Code copilot for internal development team

---

**Level 2: Emerging (Early Production)**

**Symptoms**:
- 1-3 AI tools in production, but isolated efforts
- Inconsistent governance and processes
- Pockets of success not scaled enterprise-wide
- Reactive approach to new AI capabilities

**AI Maturity**: Piloting

**Next Step**:
Create AI Center of Excellence. Establish 6-week cycle discipline. Document and share learnings.

**Focus**: Build repeatability and knowledge sharing

---

**Level 3: Scaling (Systematic Deployment)**

**Symptoms**:
- 10+ AI use cases in production
- Repeatable deployment process and governance
- Documented ROI and business impact
- 70/20/10 portfolio balance operating

**AI Maturity**: Scaling

**Next Step**:
Invest in platform capabilities (reusable components, guardrails). Prepare for Horizon 3 agentic transformation.

**Focus**: Industrialize successful patterns

---

**Level 4: AI-Native (Transformational)**

**Symptoms**:
- Business processes redesigned around AI capabilities
- Autonomous agents operating in production
- AI-powered products/services generating revenue
- Continuous innovation culture embedded

**AI Maturity**: Leading

**Next Step**:
Ecosystem innovation. Extend AI capabilities to partners, customers, suppliers.

**Focus**: AI as competitive differentiator

---

**Self-Assessment**:
Which level best describes your organization today? ______

**Target State** (12 months): ______

**Gap**: ______ levels to advance

**Speaker Notes**:
- Most organizations are Level 1 or early Level 2
- Moving one level typically takes 6-12 months of disciplined effort
- Can't skip levels‚Äîmust build capability progressively
- Ask: "Show of hands, who's at Level 1?" (likely majority)

---

### Slide 28: Success Metrics Framework

**Visual**: Three-tier metrics pyramid

**Input Metrics (Leading Indicators)**

**Innovation Velocity**:
- Number of Lightning Pilots launched per quarter (Target: 15-20)
- Average time from idea to production (Target: ‚â§6 weeks)
- % of pilots that redesign workflows (Target: >80%)

**Resource Allocation**:
- % of innovation budget on AI (Target: 40-60%)
- Cross-functional team composition
- Executive sponsorship strength

**Governance Health**:
- Time to approval for <$100K pilots (Target: ‚â§5 days)
- % of pilots blocked by governance (Target: <10%)

---

**Process Metrics (Execution Health)**

**Cycle Discipline**:
- % of pilots completing in 6 weeks (Target: >70%)
- Decision velocity: Days from pilot to scale/kill (Target: ‚â§7 days)
- % of killed pilots with documented learnings (Target: 100%)

**Quality Indicators**:
- User adoption within pilot groups (Target: >50%)
- % of pilots requiring major rework (Target: <20%)

**Learning Culture**:
- Learnings shared across teams (# documented)
- Repeat failure rate (Target: <10%)

---

**Output Metrics (Business Impact)**

**Production Deployment**:
- % of pilots reaching production (Target: 40-60%)
- Total AI use cases in production (Target: Double every 6 months)
- Scale rate: Pilots ‚Üí Enterprise (Target: 50%)

**Financial Impact**:
- ROI per successful pilot (Target: 3-5x within 6 months)
- Revenue increase or cost reduction ($)
- P&L impact within 6 months (Target: >50% of pilots)

**Strategic Progress**:
- Organizational readiness level (1-4)
- Portfolio balance: 70/20/10 adherence
- AI capability build (platform components)

---

**The Metrics Philosophy**:
Focus on **fewer high-value metrics** aligned to strategic goals, not tracking everything.

**Avoid Vanity Metrics**:
‚ùå # of AI ideas submitted
‚ùå # of employees trained on AI
‚ùå # of POCs running

**Focus on Outcomes**:
‚úÖ Production deployments with ROI
‚úÖ Business impact (time, cost, revenue)
‚úÖ User adoption and satisfaction

**Speaker Notes**:
- Don't try to measure all of these‚Äîpick 3-5 per tier
- Input metrics are easiest but least valuable
- Output metrics hardest to measure but most important
- Balance leading (input) and lagging (output) indicators

---

### Slide 29: Common Pitfalls to Avoid

**Visual**: Warning signs with red flags

**Pitfall #1: The "Big Bang" AI Transformation**
‚ùå Announcing enterprise-wide AI transformation without tactical pilots
‚úÖ Start with narrow, high-ROI use cases. Let success speak for itself.

**Pitfall #2: Technology-First Approach**
‚ùå "We bought AI tools, now let's find uses for them"
‚úÖ Start with business problems, then find AI solutions.

**Pitfall #3: Pilot Purgatory**
‚ùå Endless POCs that never reach production
‚úÖ Redefine pilots as limited-scope production deployments. 6-week deadline for go/kill.

**Pitfall #4: Governance Theater**
‚ùå Stage-Gate reviews without real decision-making
‚úÖ Risk-proportionate governance. Fast-track approvals for low-risk experiments.

**Pitfall #5: The "Let IT Handle It" Trap**
‚ùå Treating AI as IT project, not business transformation
‚úÖ Cross-functional teams from day 1. Business owns outcomes.

**Pitfall #6: Ignoring Organizational Readiness**
‚ùå Deploying AI without change management
‚úÖ Organizational readiness assessment before scaling. Invest in training, culture.

**Pitfall #7: ROI Measurement Failure**
‚ùå Can't prove business impact of AI initiatives
‚úÖ Define measurable business metrics before pilot starts. Track religiously.

**The Pattern**:
All pitfalls stem from applying **traditional innovation thinking** to AI (which requires speed and experimentation).

**Speaker Notes**:
- Ask: "Which pitfall is most likely to trip up your organization?"
- Most will recognize 2-3 of these from experience
- Awareness is first step to avoidance

---

### Slide 30: EXERCISE 4 - Your 90-Day Action Plan

**Instructions**:

Create your personal action plan for the next 90 days.

**Week 1-2: Assessment & Mobilization**

1. **Organizational Readiness**: What level are we at today? ______
2. **Top Use Case**: What's our highest-priority Lightning Pilot? ______________________
3. **Key Stakeholder**: Who needs to sponsor this? ______________________
4. **Biggest Governance Blocker**: What approval process needs fixing? ______________________

**Week 3-8: Lightning Pilot Preparation**

5. **Cross-Functional Team**: Who needs to be on the team?
   - Business: ______________________
   - IT: ______________________
   - Data Science: ______________________

6. **Success Metric**: What measurable outcome defines success? ______________________

**Week 9-12: Learning & Scaling**

7. **Knowledge Sharing**: How will we document and share learnings? ______________________

8. **One Bold Action**: What's one thing I can do Monday morning to start this journey?
   ______________________________________________________________________________________

**Time**: 8 minutes (silent individual work)

**Optional Sharing** (if time allows):
"Who wants to share their 'one bold action'?" (2-3 volunteers)

**Speaker Notes**:
- This makes the workshop actionable‚Äînot just inspiration
- Encourage specificity (names, dates, metrics)
- "One bold action" creates momentum Monday morning
- Collect worksheets for CPE documentation

---

## SECTION 5: WRAP-UP & NEXT STEPS

### Slide 31: Key Takeaways

**Visual**: Key icon with 7 numbered points

**Today's Key Takeaways**:

1Ô∏è‚É£ **The AI shockwave is a TIME COMPRESSION challenge**, not a technology challenge. 6-week cycles beat 6-month cycles.

2Ô∏è‚É£ **95% of GenAI pilots fail** because organizations apply traditional innovation thinking to AI (which requires speed).

3Ô∏è‚É£ **The 5% Success Formula** has 7 factors:
   - Tactical Clarity + Compressed Cycles + Workflow Redesign + Speed-Enabling Governance + Proactive Risk + Continuous Learning + Cross-Functional Collaboration

4Ô∏è‚É£ **Three-Horizon Framework** balances quick wins (H1: Lightning Pilots) with strategic transformation (H3: Agentic AI).

5Ô∏è‚É£ **Lightning Pilots** (6-week cycles, run 5 simultaneously, expect 2-3 to succeed) prevent pilot purgatory.

6Ô∏è‚É£ **Workflow redesign BEFORE AI deployment** is the hardest but most critical success factor.

7Ô∏è‚É£ **Start Monday**: Pick 1 tactical use case, get executive sponsor, commit to 6-week production deployment.

**The Big Idea**:
You don't need better AI technology to join the 5%. You need better **execution discipline**.

**Speaker Notes**:
- Recap entire workshop in 7 bullets
- Emphasize: This is about doing, not knowing
- Action Monday morning > Perfect plan next quarter

---

### Slide 32: Resources & Further Learning

**Frameworks & Tools Provided**:
‚úÖ Enterprise AI Shockwave Planning Framework (full document)
‚úÖ Use Case Prioritization Scoring Template
‚úÖ Organizational Readiness Maturity Assessment
‚úÖ 90-Day Implementation Roadmap
‚úÖ Success Metrics Framework
‚úÖ Workshop slides and workbook

**Recommended Reading**:
- üìÑ **MIT Report**: "95% of Generative AI Pilots Are Failing" (Sheryl Estrada)
- üìÑ **Menlo Ventures**: "2024 State of Generative AI in the Enterprise"
- üìÑ **McKinsey**: "State of AI Early 2024"
- üìÑ **Gartner**: "Innovation Guide for Generative AI Technologies"
- üìò **Book**: "Co-Intelligence" by Ethan Mollick

**Online Communities**:
- AI Leadership Forums
- Enterprise AI Slack/Discord channels
- LinkedIn Groups for AI transformation

**Follow-Up Support**:
- Email: [Your Email]
- LinkedIn: [Your LinkedIn]
- Office hours: [Schedule if offering]

**Speaker Notes**:
- All materials will be emailed within 24 hours
- Encourage connecting on LinkedIn for ongoing discussion
- Mention any follow-up workshops or consulting services

---

### Slide 33: Q&A and Closing

**Visual**: Question mark icon

**Open Discussion**:

**Time for Your Questions**:
- Clarifications on frameworks
- Specific challenges you're facing
- How to apply this in your context

**Before You Go**:
‚úÖ Complete the evaluation form (required for CPE credit)
‚úÖ Connect with a colleague to share your "one bold action"
‚úÖ Take a photo of the frameworks for reference
‚úÖ Schedule a follow-up conversation with your team

**Closing Quote**:
> "The best time to start was 6 months ago. The second best time is Monday morning." - Anonymous

**Thank you for participating!**

[Contact Information]

**Speaker Notes**:
- Allow 5-10 minutes for Q&A (depending on time remaining)
- Collect evaluation forms before dismissal
- Remind about CPE documentation
- Thank participants for energy and engagement
- End on inspiring note: "You can do this. Start Monday."

---

## APPENDIX: FACILITATION GUIDE

### Pre-Workshop Checklist

**1 Week Before**:
- [ ] Send pre-work email with agenda and learning objectives
- [ ] Share optional reading: MIT report, McKinsey article
- [ ] Request participants bring 2-3 potential AI use cases to discuss
- [ ] Prepare workbooks (print or digital)
- [ ] Test polling technology if using digital tools

**1 Day Before**:
- [ ] Send reminder email with logistics
- [ ] Confirm A/V setup, projector, microphones
- [ ] Print sign-in sheet for CPE documentation
- [ ] Print evaluation forms
- [ ] Prepare name tents or name tags

**Morning Of**:
- [ ] Arrive 30 minutes early
- [ ] Test slides, clicker, video if any
- [ ] Set up refreshments
- [ ] Distribute workbooks and materials
- [ ] Set up sign-in sheet at entrance

---

### Time Management Tips

**If Running Behind Schedule**:
- Shorten exercises by 2 minutes each (cut discussion time)
- Skip Slide 20 (Lightning Pilot Example)‚Äîreference it briefly
- Reduce Q&A at end to 5 minutes

**If Ahead of Schedule**:
- Extend Exercise 3 (Use Case Prioritization) for deeper work
- Add group discussion after Exercise 2 (Success Factor Gap Analysis)
- Allow more Q&A time

**Pacing Signals**:
- 30 min mark: Should be finishing Section 1
- 60 min mark: Should be finishing Section 2 (before break)
- 90 min mark: Should be finishing Section 3
- 110 min mark: Should be finishing Section 4
- 120 min mark: Closing

---

### Participant Engagement Techniques

**Opening** (First 10 minutes):
- Start with Poll (Slide 3) to activate participants
- Use chat or verbal responses for engagement
- Acknowledge range of experiences in room

**Throughout**:
- Ask "show of hands" questions frequently
- Call on 2-3 volunteers after each exercise to share
- Reference poll results from opening throughout workshop
- Use participant examples when they share

**Managing Energy**:
- Section 1 (Crisis): Create urgency, wake people up
- Section 2 (Success Formula): Build confidence, share stories
- Break: Network and connect
- Section 3 (Framework): Tactical and practical
- Section 4 (Action Plan): Inspirational, forward-looking

**Difficult Situations**:
- **Skeptic in room**: Acknowledge concerns, cite research evidence
- **Dominating participant**: Thank them, redirect to others
- **Low energy**: Add unplanned discussion, call on quiet participants
- **Too many questions**: "Great question, let's park that for the break/end"

---

### Exercise Facilitation Guide

**Exercise 1: Pilot Purgatory Self-Assessment** (5 min)
- Give clear 5-minute timer
- Walk around to gauge progress
- Don't collect‚Äîthis is personal reflection
- After: "Show of hands who scored below 30?" (gauge without shaming)

**Exercise 2: Success Factor Gap Analysis** (7 min)
- 3 minutes silent individual work
- 4 minutes pair discussion
- Call on 2-3 volunteers to share top priority
- Acknowledge common themes

**Exercise 3: Use Case Prioritization** (10 min)
- 3 min brainstorm
- 5 min scoring (circulate to help with questions)
- 2 min identify highest scorer
- Ask 2-3 to share their use case and score
- Validate scoring approach

**Exercise 4: 90-Day Action Plan** (8 min)
- This is most important‚Äîgive full time
- Silent individual work
- Optional sharing at end (if time allows)
- Collect worksheets for CPE records

---

### CPE Documentation Requirements

**Required for Credit**:
1. Sign-in sheet with participant names, dates, signatures
2. Agenda with learning objectives and time allocations
3. Completed evaluation forms
4. Instructor qualifications (bio)

**Evaluation Form Must Include**:
- Participant name and credentials
- Workshop title and date
- Learning objectives met (yes/no ratings)
- Quality ratings (content, instructor, materials)
- Open-ended feedback

**Post-Workshop**:
- Scan sign-in sheets and evaluations
- Store for 3 years (audit requirement)
- Send certificates of completion within 1 week

---

### Backup Plans

**Technology Failure**:
- Have printed slides as backup
- Exercises work on paper (don't require digital tools)
- Can run entire workshop unplugged

**Low Attendance**:
- Minimum viable: 5 participants
- More discussion time, less presentation
- Encourage peer coaching

**High Attendance** (>30 people):
- Pair discussions instead of group sharing
- Use digital polling instead of show of hands
- Skip optional sharing after exercises

---

## Document Metadata

**Workshop Title**: Navigating the Enterprise AI Shockwave: From Pilot Purgatory to Production Success

**Version**: 1.0
**Created**: 2025-10-20
**Duration**: 2 hours (120 minutes)
**CPE Credits**: 2.0 hours
**Level**: Intermediate
**Field of Study**: Information Technology / Business Management

**Source Material**:
- [[03_Resources/Enterprise AI Shockwave Planning Framework]]
- [[03_Resources/AI & Technology/Gartner Research/00 - Gartner Research Index]]
- [[01_Projects/Power BI Fabric Migration]] (related AI strategy service offering)
- [[01_Projects/2025-10-21 Nokia Innovation Presentation/Nokia Innovation Presentation]]
- [[Readwise/Books/Co-Intelligence by Ethan Mollick Highlights]]
- [[Readwise/Articles/MIT Report 95% of Generative AI Pilots at Companies Are Failing]]

**Tags**: #workshop #CPE #AI #innovation #training #presentation

---

*This presentation synthesizes evidence-based research on enterprise AI adoption into an interactive 2-hour workshop designed to help professionals navigate the AI shockwave and achieve production success.*
